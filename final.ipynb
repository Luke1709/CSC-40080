{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Iterative Imputer & Random Forest\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('city_day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Date</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>Benzene</th>\n",
       "      <th>Toluene</th>\n",
       "      <th>Xylene</th>\n",
       "      <th>AQI</th>\n",
       "      <th>AQI_Bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>18.22</td>\n",
       "      <td>17.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>27.64</td>\n",
       "      <td>133.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>15.69</td>\n",
       "      <td>16.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>24.55</td>\n",
       "      <td>34.06</td>\n",
       "      <td>3.68</td>\n",
       "      <td>5.50</td>\n",
       "      <td>3.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.40</td>\n",
       "      <td>19.30</td>\n",
       "      <td>29.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.40</td>\n",
       "      <td>29.07</td>\n",
       "      <td>30.70</td>\n",
       "      <td>6.80</td>\n",
       "      <td>16.40</td>\n",
       "      <td>2.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.70</td>\n",
       "      <td>18.48</td>\n",
       "      <td>17.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.70</td>\n",
       "      <td>18.59</td>\n",
       "      <td>36.08</td>\n",
       "      <td>4.43</td>\n",
       "      <td>10.14</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.10</td>\n",
       "      <td>21.42</td>\n",
       "      <td>37.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.10</td>\n",
       "      <td>39.33</td>\n",
       "      <td>39.31</td>\n",
       "      <td>7.01</td>\n",
       "      <td>18.89</td>\n",
       "      <td>2.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City        Date  PM2.5  PM10     NO    NO2    NOx  NH3     CO    SO2  \\\n",
       "0  Ahmedabad  2015-01-01    NaN   NaN   0.92  18.22  17.15  NaN   0.92  27.64   \n",
       "1  Ahmedabad  2015-01-02    NaN   NaN   0.97  15.69  16.46  NaN   0.97  24.55   \n",
       "2  Ahmedabad  2015-01-03    NaN   NaN  17.40  19.30  29.70  NaN  17.40  29.07   \n",
       "3  Ahmedabad  2015-01-04    NaN   NaN   1.70  18.48  17.97  NaN   1.70  18.59   \n",
       "4  Ahmedabad  2015-01-05    NaN   NaN  22.10  21.42  37.76  NaN  22.10  39.33   \n",
       "\n",
       "       O3  Benzene  Toluene  Xylene  AQI AQI_Bucket  \n",
       "0  133.36     0.00     0.02    0.00  NaN        NaN  \n",
       "1   34.06     3.68     5.50    3.77  NaN        NaN  \n",
       "2   30.70     6.80    16.40    2.25  NaN        NaN  \n",
       "3   36.08     4.43    10.14    1.00  NaN        NaN  \n",
       "4   39.31     7.01    18.89    2.78  NaN        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drop AQI_Bucket, not needed for this task\n",
    "- Drop any rows missing AQI values from simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset from raw data, dropping AQI Bucket\n",
    "data = raw_data.drop(['AQI_Bucket'], axis=1)\n",
    "\n",
    "# Dropping rows with missing AQI values\n",
    "data = data.dropna(subset=['AQI'])\n",
    "\n",
    "# Convert the date to correct format\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce the data to 3 cities to reduce geographical variation: Jaipur, Amritsar, Thiruvananthapuram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['Amritsar', 'Amaravati', 'Jaipur']\n",
    "data = data[data['City'].isin(regions)]\n",
    "\n",
    "# Reset Index\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>Benzene</th>\n",
       "      <th>Toluene</th>\n",
       "      <th>Xylene</th>\n",
       "      <th>AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3061</td>\n",
       "      <td>2971.000</td>\n",
       "      <td>3031.000</td>\n",
       "      <td>3006.000</td>\n",
       "      <td>3049.000</td>\n",
       "      <td>2727.000</td>\n",
       "      <td>3045.000</td>\n",
       "      <td>2979.000</td>\n",
       "      <td>2921.000</td>\n",
       "      <td>2976.000</td>\n",
       "      <td>2872.000</td>\n",
       "      <td>2855.000</td>\n",
       "      <td>1617.000</td>\n",
       "      <td>3061.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019-01-04 22:02:23.482522112</td>\n",
       "      <td>50.159</td>\n",
       "      <td>107.412</td>\n",
       "      <td>13.281</td>\n",
       "      <td>24.375</td>\n",
       "      <td>30.779</td>\n",
       "      <td>17.991</td>\n",
       "      <td>0.665</td>\n",
       "      <td>11.003</td>\n",
       "      <td>35.744</td>\n",
       "      <td>2.194</td>\n",
       "      <td>4.145</td>\n",
       "      <td>4.885</td>\n",
       "      <td>118.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2017-02-28 00:00:00</td>\n",
       "      <td>2.850</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2018-04-13 00:00:00</td>\n",
       "      <td>28.375</td>\n",
       "      <td>62.905</td>\n",
       "      <td>4.880</td>\n",
       "      <td>11.640</td>\n",
       "      <td>15.945</td>\n",
       "      <td>9.720</td>\n",
       "      <td>0.440</td>\n",
       "      <td>7.210</td>\n",
       "      <td>21.210</td>\n",
       "      <td>0.280</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.150</td>\n",
       "      <td>74.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2019-01-16 00:00:00</td>\n",
       "      <td>43.630</td>\n",
       "      <td>97.700</td>\n",
       "      <td>10.400</td>\n",
       "      <td>18.960</td>\n",
       "      <td>26.780</td>\n",
       "      <td>14.270</td>\n",
       "      <td>0.660</td>\n",
       "      <td>10.250</td>\n",
       "      <td>31.420</td>\n",
       "      <td>1.130</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.000</td>\n",
       "      <td>104.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2019-10-17 00:00:00</td>\n",
       "      <td>63.585</td>\n",
       "      <td>138.735</td>\n",
       "      <td>16.555</td>\n",
       "      <td>32.030</td>\n",
       "      <td>39.130</td>\n",
       "      <td>23.070</td>\n",
       "      <td>0.860</td>\n",
       "      <td>13.250</td>\n",
       "      <td>46.240</td>\n",
       "      <td>2.850</td>\n",
       "      <td>5.515</td>\n",
       "      <td>8.130</td>\n",
       "      <td>143.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020-07-01 00:00:00</td>\n",
       "      <td>868.660</td>\n",
       "      <td>917.080</td>\n",
       "      <td>103.440</td>\n",
       "      <td>237.270</td>\n",
       "      <td>150.960</td>\n",
       "      <td>129.460</td>\n",
       "      <td>3.830</td>\n",
       "      <td>67.260</td>\n",
       "      <td>172.280</td>\n",
       "      <td>53.890</td>\n",
       "      <td>76.320</td>\n",
       "      <td>137.450</td>\n",
       "      <td>869.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>35.894</td>\n",
       "      <td>62.526</td>\n",
       "      <td>12.981</td>\n",
       "      <td>17.878</td>\n",
       "      <td>22.030</td>\n",
       "      <td>12.889</td>\n",
       "      <td>0.440</td>\n",
       "      <td>5.941</td>\n",
       "      <td>20.465</td>\n",
       "      <td>3.328</td>\n",
       "      <td>5.079</td>\n",
       "      <td>8.077</td>\n",
       "      <td>65.766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Date     PM2.5      PM10        NO       NO2  \\\n",
       "count                           3061  2971.000  3031.000  3006.000  3049.000   \n",
       "mean   2019-01-04 22:02:23.482522112    50.159   107.412    13.281    24.375   \n",
       "min              2017-02-28 00:00:00     2.850     0.420     0.250     0.010   \n",
       "25%              2018-04-13 00:00:00    28.375    62.905     4.880    11.640   \n",
       "50%              2019-01-16 00:00:00    43.630    97.700    10.400    18.960   \n",
       "75%              2019-10-17 00:00:00    63.585   138.735    16.555    32.030   \n",
       "max              2020-07-01 00:00:00   868.660   917.080   103.440   237.270   \n",
       "std                              NaN    35.894    62.526    12.981    17.878   \n",
       "\n",
       "            NOx       NH3        CO       SO2        O3   Benzene   Toluene  \\\n",
       "count  2727.000  3045.000  2979.000  2921.000  2976.000  2872.000  2855.000   \n",
       "mean     30.779    17.991     0.665    11.003    35.744     2.194     4.145   \n",
       "min       0.860     0.060     0.000     0.710     0.240     0.000     0.000   \n",
       "25%      15.945     9.720     0.440     7.210    21.210     0.280     1.100   \n",
       "50%      26.780    14.270     0.660    10.250    31.420     1.130     2.240   \n",
       "75%      39.130    23.070     0.860    13.250    46.240     2.850     5.515   \n",
       "max     150.960   129.460     3.830    67.260   172.280    53.890    76.320   \n",
       "std      22.030    12.889     0.440     5.941    20.465     3.328     5.079   \n",
       "\n",
       "         Xylene       AQI  \n",
       "count  1617.000  3061.000  \n",
       "mean      4.885   118.074  \n",
       "min       0.000    20.000  \n",
       "25%       0.150    74.000  \n",
       "50%       2.000   104.000  \n",
       "75%       8.130   143.000  \n",
       "max     137.450   869.000  \n",
       "std       8.077    65.766  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data summaries \n",
    "np.round(data.describe(),3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City          0\n",
       "Date          0\n",
       "PM2.5        90\n",
       "PM10         30\n",
       "NO           55\n",
       "NO2          12\n",
       "NOx         334\n",
       "NH3          16\n",
       "CO           82\n",
       "SO2         140\n",
       "O3           85\n",
       "Benzene     189\n",
       "Toluene     206\n",
       "Xylene     1444\n",
       "AQI           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As per EDA, drop Benzene, Toluene, Xylene as these do not directly correlate with AQI and have significant numbers of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Benzene, Toluene, Xylene from data\n",
    "data.drop(['Benzene', 'Toluene', 'Xylene'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add missing Values using Iterative Imputer package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PM2.5    0\n",
      "PM10     0\n",
      "NO       0\n",
      "NO2      0\n",
      "NOx      0\n",
      "NH3      0\n",
      "CO       0\n",
      "SO2      0\n",
      "O3       0\n",
      "AQI      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Drop the 'City' and 'Date' column, as it's not used for imputation\n",
    "data_model = data.drop(['City', 'Date'], axis=1)\n",
    "\n",
    "# Initialize the Iterative Imputer with a RandomForestRegressor\n",
    "imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)\n",
    "\n",
    "# Apply the imputer to the dataset\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(data_model), columns=data_model.columns)\n",
    "\n",
    "# Display information about missing values after imputation\n",
    "print(df_imputed.isnull().sum())\n",
    "\n",
    "# Merge the imputed dataframe and the dataset\n",
    "data_new = pd.concat([data[['City', 'Date']], df_imputed], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets\n",
    "amarvati = pd.read_csv('amarvati.csv')\n",
    "amritsar = pd.read_csv('amritsar.csv')\n",
    "jaipur = pd.read_csv('jaipur.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' columns to datetime and remove timezones for consistency\n",
    "data_new['Date'] = pd.to_datetime(data_new['Date']).dt.tz_localize(None)\n",
    "amarvati['date'] = pd.to_datetime(amarvati['date']).dt.tz_localize(None)\n",
    "jaipur['date'] = pd.to_datetime(jaipur['date']).dt.tz_localize(None)\n",
    "amritsar['date'] = pd.to_datetime(amritsar['date']).dt.tz_localize(None)\n",
    "\n",
    "# Rename the 'date' columns to 'Date' for consistency\n",
    "amarvati.rename(columns={'date': 'Date'}, inplace=True)\n",
    "jaipur.rename(columns={'date': 'Date'}, inplace=True)\n",
    "amritsar.rename(columns={'date': 'Date'}, inplace=True)\n",
    "\n",
    "# Add the 'City' column to each weather dataset\n",
    "amarvati['City'] = 'Amaravati'\n",
    "jaipur['City'] = 'Jaipur'\n",
    "amritsar['City'] = 'Amritsar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Date</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>...</th>\n",
       "      <th>humidity_max</th>\n",
       "      <th>dew_point_mean</th>\n",
       "      <th>dew_point_max</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>rain_sum</th>\n",
       "      <th>cloud_cover_sum</th>\n",
       "      <th>wind_speed_10m_mean</th>\n",
       "      <th>wind_speed_10m_max</th>\n",
       "      <th>wind_speed_100m_mean</th>\n",
       "      <th>wind_speed_100m_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>81.40</td>\n",
       "      <td>124.50</td>\n",
       "      <td>1.44</td>\n",
       "      <td>20.50</td>\n",
       "      <td>12.08</td>\n",
       "      <td>10.72</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.24</td>\n",
       "      <td>...</td>\n",
       "      <td>89.346016</td>\n",
       "      <td>19.013000</td>\n",
       "      <td>21.263000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>819.000009</td>\n",
       "      <td>9.180352</td>\n",
       "      <td>16.808570</td>\n",
       "      <td>15.935398</td>\n",
       "      <td>27.248455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-26</td>\n",
       "      <td>78.32</td>\n",
       "      <td>129.06</td>\n",
       "      <td>1.26</td>\n",
       "      <td>26.00</td>\n",
       "      <td>14.85</td>\n",
       "      <td>10.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>26.96</td>\n",
       "      <td>...</td>\n",
       "      <td>95.198590</td>\n",
       "      <td>18.248417</td>\n",
       "      <td>21.063000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>532.800023</td>\n",
       "      <td>7.759277</td>\n",
       "      <td>11.988594</td>\n",
       "      <td>14.097954</td>\n",
       "      <td>22.183128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>88.76</td>\n",
       "      <td>135.32</td>\n",
       "      <td>6.60</td>\n",
       "      <td>30.85</td>\n",
       "      <td>21.77</td>\n",
       "      <td>12.91</td>\n",
       "      <td>0.11</td>\n",
       "      <td>33.59</td>\n",
       "      <td>...</td>\n",
       "      <td>97.275760</td>\n",
       "      <td>18.960917</td>\n",
       "      <td>21.463001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>618.900014</td>\n",
       "      <td>6.287076</td>\n",
       "      <td>9.085988</td>\n",
       "      <td>11.361810</td>\n",
       "      <td>17.418196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>64.18</td>\n",
       "      <td>104.09</td>\n",
       "      <td>2.56</td>\n",
       "      <td>28.07</td>\n",
       "      <td>17.01</td>\n",
       "      <td>11.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>19.00</td>\n",
       "      <td>...</td>\n",
       "      <td>94.871315</td>\n",
       "      <td>17.425500</td>\n",
       "      <td>21.013000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>378.000013</td>\n",
       "      <td>7.019016</td>\n",
       "      <td>13.684735</td>\n",
       "      <td>13.233190</td>\n",
       "      <td>25.630886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>72.47</td>\n",
       "      <td>114.84</td>\n",
       "      <td>5.23</td>\n",
       "      <td>23.20</td>\n",
       "      <td>16.59</td>\n",
       "      <td>12.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10.55</td>\n",
       "      <td>...</td>\n",
       "      <td>96.376900</td>\n",
       "      <td>17.617167</td>\n",
       "      <td>20.663000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.000007</td>\n",
       "      <td>10.355873</td>\n",
       "      <td>14.186923</td>\n",
       "      <td>18.022025</td>\n",
       "      <td>23.904108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        City       Date  PM2.5    PM10    NO    NO2    NOx    NH3    CO  \\\n",
       "0  Amaravati 2017-11-25  81.40  124.50  1.44  20.50  12.08  10.72  0.12   \n",
       "1  Amaravati 2017-11-26  78.32  129.06  1.26  26.00  14.85  10.28  0.14   \n",
       "2  Amaravati 2017-11-27  88.76  135.32  6.60  30.85  21.77  12.91  0.11   \n",
       "3  Amaravati 2017-11-28  64.18  104.09  2.56  28.07  17.01  11.42  0.09   \n",
       "4  Amaravati 2017-11-29  72.47  114.84  5.23  23.20  16.59  12.25  0.16   \n",
       "\n",
       "     SO2  ...  humidity_max  dew_point_mean  dew_point_max  precipitation_sum  \\\n",
       "0  15.24  ...     89.346016       19.013000      21.263000                0.0   \n",
       "1  26.96  ...     95.198590       18.248417      21.063000                0.0   \n",
       "2  33.59  ...     97.275760       18.960917      21.463001                0.0   \n",
       "3  19.00  ...     94.871315       17.425500      21.013000                0.0   \n",
       "4  10.55  ...     96.376900       17.617167      20.663000                0.0   \n",
       "\n",
       "   rain_sum  cloud_cover_sum  wind_speed_10m_mean  wind_speed_10m_max  \\\n",
       "0       0.0       819.000009             9.180352           16.808570   \n",
       "1       0.0       532.800023             7.759277           11.988594   \n",
       "2       0.0       618.900014             6.287076            9.085988   \n",
       "3       0.0       378.000013             7.019016           13.684735   \n",
       "4       0.0       189.000007            10.355873           14.186923   \n",
       "\n",
       "   wind_speed_100m_mean  wind_speed_100m_max  \n",
       "0             15.935398            27.248455  \n",
       "1             14.097954            22.183128  \n",
       "2             11.361810            17.418196  \n",
       "3             13.233190            25.630886  \n",
       "4             18.022025            23.904108  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to map cities to the correct city csv file\n",
    "city_data = {\n",
    "    'Amaravati': amarvati,\n",
    "    'Jaipur': jaipur,\n",
    "    'Amritsar': amritsar\n",
    "}\n",
    "\n",
    "# Initialise empty lists for storing new data\n",
    "temperature_mean = []\n",
    "temperature_max = []\n",
    "humidity_mean = []\n",
    "humidity_max = []\n",
    "dew_point_mean = []\n",
    "dew_point_max = []\n",
    "precipitation_sum = []\n",
    "rain_sum = []\n",
    "cloud_cover_sum = []\n",
    "wind_speed_10m_mean = []\n",
    "wind_speed_10m_max = []\n",
    "wind_speed_100m_mean = []\n",
    "wind_speed_100m_max = []\n",
    "\n",
    "# Loop over each row in data_new_df\n",
    "for index, row in data_new.iterrows():\n",
    "    city = row['City']\n",
    "    date = row['Date']\n",
    "    \n",
    "    # Check if the city has corresponding weather data\n",
    "    if city in city_data:\n",
    "        weather_df = city_data[city]\n",
    "        # Find the matching row in the city's weather DataFrame\n",
    "        match = weather_df[weather_df['Date'] == date]\n",
    "        \n",
    "        if not match.empty:\n",
    "            # Append values to the lists if a match is found\n",
    "            temperature_mean.append(match['temperature_mean'].values[0])\n",
    "            temperature_max.append(match['temperature_max'].values[0])\n",
    "            humidity_mean.append(match['humidity_mean'].values[0])\n",
    "            humidity_max.append(match['humidity_max'].values[0])\n",
    "            dew_point_mean.append(match['dew_point_mean'].values[0])\n",
    "            dew_point_max.append(match['dew_point_max'].values[0])\n",
    "            precipitation_sum.append(match['precipitation_sum'].values[0])\n",
    "            rain_sum.append(match['rain_sum'].values[0])\n",
    "            cloud_cover_sum.append(match['cloud_cover_sum'].values[0])\n",
    "            wind_speed_10m_mean.append(match['wind_speed_10m_mean'].values[0])\n",
    "            wind_speed_10m_max.append(match['wind_speed_10m_max'].values[0])\n",
    "            wind_speed_100m_mean.append(match['wind_speed_100m_mean'].values[0])\n",
    "            wind_speed_100m_max.append(match['wind_speed_100m_max'].values[0])\n",
    "        else:\n",
    "            # Append NaN if no match is found\n",
    "            temperature_mean.append(float('nan'))\n",
    "            temperature_max.append(float('nan'))\n",
    "            humidity_mean.append(float('nan'))\n",
    "            humidity_max.append(float('nan'))\n",
    "            dew_point_mean.append(float('nan'))\n",
    "            dew_point_max.append(float('nan'))\n",
    "            precipitation_sum.append(float('nan'))\n",
    "            rain_sum.append(float('nan'))\n",
    "            cloud_cover_sum.append(float('nan'))\n",
    "            wind_speed_10m_mean.append(float('nan'))\n",
    "            wind_speed_10m_max.append(float('nan'))\n",
    "            wind_speed_100m_mean.append(float('nan'))\n",
    "            wind_speed_100m_max.append(float('nan'))\n",
    "    else:\n",
    "        # Append NaN if no matching city is found\n",
    "        temperature_mean.append(float('nan'))\n",
    "        temperature_max.append(float('nan'))\n",
    "        humidity_mean.append(float('nan'))\n",
    "        humidity_max.append(float('nan'))\n",
    "        dew_point_mean.append(float('nan'))\n",
    "        dew_point_max.append(float('nan'))\n",
    "        precipitation_sum.append(float('nan'))\n",
    "        rain_sum.append(float('nan'))\n",
    "        cloud_cover_sum.append(float('nan'))\n",
    "        wind_speed_10m_mean.append(float('nan'))\n",
    "        wind_speed_10m_max.append(float('nan'))\n",
    "        wind_speed_100m_mean.append(float('nan'))\n",
    "        wind_speed_100m_max.append(float('nan'))\n",
    "\n",
    "# Add the new data to the DataFrame\n",
    "data_new['temperature_mean'] = temperature_mean\n",
    "data_new['temperature_max'] = temperature_max\n",
    "data_new['humidity_mean'] = humidity_mean\n",
    "data_new['humidity_max'] = humidity_max\n",
    "data_new['dew_point_mean'] = dew_point_mean\n",
    "data_new['dew_point_max'] = dew_point_max\n",
    "data_new['precipitation_sum'] = precipitation_sum\n",
    "data_new['rain_sum'] = rain_sum\n",
    "data_new['cloud_cover_sum'] = cloud_cover_sum\n",
    "data_new['wind_speed_10m_mean'] = wind_speed_10m_mean\n",
    "data_new['wind_speed_10m_max'] = wind_speed_10m_max\n",
    "data_new['wind_speed_100m_mean'] = wind_speed_100m_mean\n",
    "data_new['wind_speed_100m_max'] = wind_speed_100m_max\n",
    "\n",
    "data_new.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Ridge & Lasso Regression to examine features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "X = data_new[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'temperature_mean', 'temperature_max', 'humidity_mean', 'humidity_max', 'dew_point_mean', 'dew_point_max', 'precipitation_sum', 'rain_sum', 'cloud_cover_sum', 'wind_speed_10m_mean', 'wind_speed_10m_max', 'wind_speed_100m_mean', 'wind_speed_100m_max']]\n",
    "y = data_new['AQI'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 714.4169339270956\n",
      "R-squared: 0.8104656944654502\n",
      "                 Feature  Coefficient\n",
      "0                  PM2.5      18.9057\n",
      "1                   PM10      42.8018\n",
      "2                     NO       3.6629\n",
      "3                    NO2       4.9293\n",
      "4                    NOx      -3.1415\n",
      "5                    NH3      -2.1444\n",
      "6                     CO       3.5059\n",
      "7                    SO2       0.0241\n",
      "8                     O3       6.3924\n",
      "9       temperature_mean       0.6420\n",
      "10       temperature_max       0.0835\n",
      "11         humidity_mean       2.2021\n",
      "12          humidity_max       1.9042\n",
      "13        dew_point_mean       3.8472\n",
      "14         dew_point_max      -5.6242\n",
      "15     precipitation_sum      -0.2037\n",
      "16              rain_sum      -0.2037\n",
      "17       cloud_cover_sum       0.3300\n",
      "18   wind_speed_10m_mean       2.0496\n",
      "19    wind_speed_10m_max      -1.9227\n",
      "20  wind_speed_100m_mean       0.2067\n",
      "21   wind_speed_100m_max       1.5338\n",
      "Cross-validated MSE (for each fold): [ 783.99640614 2134.24207405 1079.16714514  773.17962408  614.56461681]\n",
      "Average MSE: 1077.0299732451222\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardise both sets of data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initiate Ridge Regression\n",
    "ridge = Ridge(alpha=1) \n",
    "\n",
    "# Fit the model\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(ridge.coef_,4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "ridge_cv = cross_val_score(ridge, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "                           \n",
    "# Output the results (MSE for each fold)\n",
    "print(\"Cross-validated MSE (for each fold):\", -ridge_cv)\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -ridge_cv.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Ridge: 100\n",
      "Mean Squared Error: 707.0087179608779\n",
      "R-squared: 0.8124310889035254\n",
      "                 Feature  Coefficient\n",
      "0                  PM2.5      19.4185\n",
      "1                   PM10      40.2600\n",
      "2                     NO       2.7101\n",
      "3                    NO2       4.0744\n",
      "4                    NOx      -0.9658\n",
      "5                    NH3      -1.8596\n",
      "6                     CO       3.6083\n",
      "7                    SO2       0.0206\n",
      "8                     O3       6.3842\n",
      "9       temperature_mean      -0.1535\n",
      "10       temperature_max      -0.2985\n",
      "11         humidity_mean       1.5207\n",
      "12          humidity_max       1.7278\n",
      "13        dew_point_mean       1.5762\n",
      "14         dew_point_max      -2.2845\n",
      "15     precipitation_sum      -0.1465\n",
      "16              rain_sum      -0.1465\n",
      "17       cloud_cover_sum       0.0597\n",
      "18   wind_speed_10m_mean       1.2185\n",
      "19    wind_speed_10m_max      -1.1690\n",
      "20  wind_speed_100m_mean       0.6705\n",
      "21   wind_speed_100m_max       0.9120\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the alpha values to test for tuning\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Initiate Ridge Regression and GridSearchCV\n",
    "ridge = Ridge()\n",
    "\n",
    "# Use GridSearchCV to tune alpha\n",
    "ridge_cv = GridSearchCV(ridge, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best alpha value\n",
    "best_alpha = ridge_cv.best_params_['alpha']\n",
    "print(f\"Best alpha for Ridge: {best_alpha}\")\n",
    "\n",
    "# Use the best alpha to fit the model again\n",
    "ridge_best = Ridge(alpha=best_alpha)\n",
    "ridge_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge_best.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge_best.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(ridge_best.coef_, 4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 711.9714276350497\n",
      "R-squared: 0.8111144855491041\n",
      "                 Feature  Coefficient\n",
      "0                  PM2.5      18.8981\n",
      "1                   PM10      42.7007\n",
      "2                     NO       2.6002\n",
      "3                    NO2       4.0182\n",
      "4                    NOx      -1.4670\n",
      "5                    NH3      -1.9520\n",
      "6                     CO       3.3555\n",
      "7                    SO2       0.0714\n",
      "8                     O3       6.3253\n",
      "9       temperature_mean      -0.0000\n",
      "10       temperature_max      -0.0000\n",
      "11         humidity_mean       2.4166\n",
      "12          humidity_max       1.3970\n",
      "13        dew_point_mean       0.0288\n",
      "14         dew_point_max      -1.2182\n",
      "15     precipitation_sum      -0.1874\n",
      "16              rain_sum      -0.0000\n",
      "17       cloud_cover_sum       0.1439\n",
      "18   wind_speed_10m_mean       0.4374\n",
      "19    wind_speed_10m_max      -0.3163\n",
      "20  wind_speed_100m_mean       1.5439\n",
      "21   wind_speed_100m_max       0.0593\n",
      "Cross-validated MSE (for each fold): [ 780.04058516 2137.45702153 1077.86728909  770.28176955  608.52467528]\n",
      "Average MSE: 1074.83426812311\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Using majority of the same code as above:\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = lasso.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(lasso.coef_,4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "lasso_cv = cross_val_score(lasso, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "                           \n",
    "# Output the results (MSE for each fold)\n",
    "print(\"Cross-validated MSE (for each fold):\", -lasso_cv)\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -lasso_cv.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Lasso: 1\n",
      "Mean Squared Error: 707.0963113417124\n",
      "R-squared: 0.8124310889035254\n",
      "                 Feature  Coefficient\n",
      "0                  PM2.5      18.1992\n",
      "1                   PM10      42.3390\n",
      "2                     NO       0.4900\n",
      "3                    NO2       2.4493\n",
      "4                    NOx       0.0000\n",
      "5                    NH3      -0.0000\n",
      "6                     CO       2.8441\n",
      "7                    SO2       0.0000\n",
      "8                     O3       5.0146\n",
      "9       temperature_mean      -0.0000\n",
      "10       temperature_max      -1.1129\n",
      "11         humidity_mean       0.0000\n",
      "12          humidity_max       1.2791\n",
      "13        dew_point_mean      -0.0000\n",
      "14         dew_point_max      -0.0000\n",
      "15     precipitation_sum       0.0000\n",
      "16              rain_sum       0.0000\n",
      "17       cloud_cover_sum       0.0000\n",
      "18   wind_speed_10m_mean       0.0000\n",
      "19    wind_speed_10m_max       0.0000\n",
      "20  wind_speed_100m_mean       0.2479\n",
      "21   wind_speed_100m_max       0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the alpha values to test for tuning\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Initiate Ridge Regression and GridSearchCV\n",
    "lasso = Lasso()\n",
    "\n",
    "# Use GridSearchCV to tune alpha\n",
    "lasso_cv = GridSearchCV(lasso, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best alpha value\n",
    "best_alpha = lasso_cv.best_params_['alpha']\n",
    "print(f\"Best alpha for Lasso: {best_alpha}\")\n",
    "\n",
    "# Use the best alpha to fit the model again\n",
    "lasso_best = Lasso(alpha=best_alpha)\n",
    "lasso_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lasso_best.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge_best.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(lasso_best.coef_, 4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As PM2.5 particles are included in PM10, we face the issue of multicolinearity. By applying PCA onto these features we can reduce this.\n",
    "\n",
    "- This improves the Ridge and Lasso Regression models but will sacrifice interpretability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.8523519 0.1476481]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8h/gtndp3dj3pl2vs4brgn9zq9h0000gp/T/ipykernel_58055/408682948.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.drop(['PM2.5', 'PM10'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "pm_data_scaled = scaler.fit_transform(data_new[['PM2.5', 'PM10']])\n",
    "\n",
    "# Step 2: Apply PCA to PM2.5 and PM10\n",
    "pca = PCA(n_components=2)  # Use 2 components because we have 2 features\n",
    "pm_pca = pca.fit_transform(pm_data_scaled)\n",
    "\n",
    "# Create a new DataFrame with the principal components\n",
    "pm_pca_df = pd.DataFrame(pm_pca, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Explained variance to understand how much information is captured by each component\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance Ratio: {explained_variance}\")\n",
    "\n",
    "# Keep PC1 and drop both PM2.5 and PM10\n",
    "X.drop(['PM2.5', 'PM10'], axis=1, inplace=True)\n",
    "\n",
    "# Add PC1\n",
    "X.insert(0, 'PC1', pm_pca[:, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 626.3909068694403\n",
      "R-squared: 0.8338189369699746\n",
      "                 Feature  Coefficient\n",
      "0                    PC1      55.7678\n",
      "1                     NO       3.5996\n",
      "2                    NO2       4.8441\n",
      "3                    NOx      -1.7451\n",
      "4                    NH3      -2.5594\n",
      "5                     CO       4.7192\n",
      "6                    SO2      -1.3839\n",
      "7                     O3       7.1327\n",
      "8       temperature_mean      -5.6054\n",
      "9        temperature_max       0.7080\n",
      "10         humidity_mean      -1.6028\n",
      "11          humidity_max       1.3455\n",
      "12        dew_point_mean       8.5489\n",
      "13         dew_point_max      -3.2144\n",
      "14     precipitation_sum       0.0746\n",
      "15              rain_sum       0.0746\n",
      "16       cloud_cover_sum      -0.8852\n",
      "17   wind_speed_10m_mean       2.6621\n",
      "18    wind_speed_10m_max      -1.6514\n",
      "19  wind_speed_100m_mean      -0.9895\n",
      "20   wind_speed_100m_max       2.4133\n",
      "Cross-validated MSE (for each fold): [ 665.44373006 1824.39328851 1183.4482088   705.44207689  526.8570906 ]\n",
      "Average MSE: 981.1168789744357\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardise both sets of data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initiate Ridge Regression\n",
    "ridge = Ridge(alpha=1.0) \n",
    "\n",
    "# Fit the model\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(ridge.coef_,4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "ridge_cv = cross_val_score(ridge, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "                           \n",
    "# Output the results (MSE for each fold)\n",
    "print(\"Cross-validated MSE (for each fold):\", -ridge_cv)\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -ridge_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Ridge: 10\n",
      "Mean Squared Error: 627.0954798966924\n",
      "R-squared: 0.8336320142458945\n",
      "                 Feature  Coefficient\n",
      "0                    PC1      55.4850\n",
      "1                     NO       3.5206\n",
      "2                    NO2       4.7602\n",
      "3                    NOx      -1.4992\n",
      "4                    NH3      -2.5301\n",
      "5                     CO       4.7242\n",
      "6                    SO2      -1.3633\n",
      "7                     O3       7.1461\n",
      "8       temperature_mean      -3.1216\n",
      "9        temperature_max       0.0334\n",
      "10         humidity_mean      -0.2809\n",
      "11          humidity_max       1.4375\n",
      "12        dew_point_mean       6.1820\n",
      "13         dew_point_max      -2.8390\n",
      "14     precipitation_sum       0.0497\n",
      "15              rain_sum       0.0497\n",
      "16       cloud_cover_sum      -1.0361\n",
      "17   wind_speed_10m_mean       2.5007\n",
      "18    wind_speed_10m_max      -1.5048\n",
      "19  wind_speed_100m_mean      -0.8229\n",
      "20   wind_speed_100m_max       2.2734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the alpha values to test for tuning\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Initiate Ridge Regression and GridSearchCV\n",
    "ridge = Ridge()\n",
    "\n",
    "# Use GridSearchCV to tune alpha\n",
    "ridge_cv = GridSearchCV(ridge, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best alpha value\n",
    "best_alpha = ridge_cv.best_params_['alpha']\n",
    "print(f\"Best alpha for Ridge: {best_alpha}\")\n",
    "\n",
    "# Use the best alpha to fit the model again\n",
    "ridge_best = Ridge(alpha=best_alpha)\n",
    "ridge_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge_best.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge_best.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(ridge_best.coef_, 4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 623.6812655007577\n",
      "R-squared: 0.8345378029019981\n",
      "                 Feature  Coefficient\n",
      "0                    PC1      55.7049\n",
      "1                     NO       2.6619\n",
      "2                    NO2       3.8876\n",
      "3                    NOx      -0.0000\n",
      "4                    NH3      -2.4069\n",
      "5                     CO       4.5007\n",
      "6                    SO2      -1.1456\n",
      "7                     O3       7.0677\n",
      "8       temperature_mean      -0.0000\n",
      "9        temperature_max      -0.5331\n",
      "10         humidity_mean       1.6947\n",
      "11          humidity_max       1.4038\n",
      "12        dew_point_mean       0.5342\n",
      "13         dew_point_max      -0.0000\n",
      "14     precipitation_sum      -0.0000\n",
      "15              rain_sum      -0.0000\n",
      "16       cloud_cover_sum      -0.7746\n",
      "17   wind_speed_10m_mean       1.3476\n",
      "18    wind_speed_10m_max      -0.2351\n",
      "19  wind_speed_100m_mean       0.1106\n",
      "20   wind_speed_100m_max       1.0837\n",
      "Cross-validated MSE (for each fold): [ 661.80390764 1827.17121506 1183.26390287  701.31441498  522.01583782]\n",
      "Average MSE: 979.1138556750213\n"
     ]
    }
   ],
   "source": [
    "# Using majority of the same code as above:\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = lasso.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(lasso.coef_,4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "lasso_cv = cross_val_score(lasso, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "                           \n",
    "# Output the results (MSE for each fold)\n",
    "print(\"Cross-validated MSE (for each fold):\", -lasso_cv)\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -lasso_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Lasso: 0.1\n",
      "Mean Squared Error: 623.6812655007577\n",
      "R-squared: 0.8336320142458945\n",
      "                 Feature  Coefficient\n",
      "0                    PC1      55.7049\n",
      "1                     NO       2.6619\n",
      "2                    NO2       3.8876\n",
      "3                    NOx      -0.0000\n",
      "4                    NH3      -2.4069\n",
      "5                     CO       4.5007\n",
      "6                    SO2      -1.1456\n",
      "7                     O3       7.0677\n",
      "8       temperature_mean      -0.0000\n",
      "9        temperature_max      -0.5331\n",
      "10         humidity_mean       1.6947\n",
      "11          humidity_max       1.4038\n",
      "12        dew_point_mean       0.5342\n",
      "13         dew_point_max      -0.0000\n",
      "14     precipitation_sum      -0.0000\n",
      "15              rain_sum      -0.0000\n",
      "16       cloud_cover_sum      -0.7746\n",
      "17   wind_speed_10m_mean       1.3476\n",
      "18    wind_speed_10m_max      -0.2351\n",
      "19  wind_speed_100m_mean       0.1106\n",
      "20   wind_speed_100m_max       1.0837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the alpha values to test for tuning\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Initiate Ridge Regression and GridSearchCV\n",
    "lasso = Lasso()\n",
    "\n",
    "# Use GridSearchCV to tune alpha\n",
    "lasso_cv = GridSearchCV(lasso, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best alpha value\n",
    "best_alpha = lasso_cv.best_params_['alpha']\n",
    "print(f\"Best alpha for Lasso: {best_alpha}\")\n",
    "\n",
    "# Use the best alpha to fit the model again\n",
    "lasso_best = Lasso(alpha=best_alpha)\n",
    "lasso_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lasso_best.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge_best.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(lasso_best.coef_, 4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From here we can look to remove x,y and z features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off of Ridge & Lasso Regression, we can remove certain components\n",
    "columns_to_drop = ['NOx', 'temperature_mean', 'dew_point_max', 'precipitation_sum', 'rain_sum', 'wind_speed_100m_mean', 'wind_speed_10m_max', 'temperature_max', 'dew_point_mean'] # Removing all zeroed components & any low coefficient features\n",
    "\n",
    "# Drop columns on original data\n",
    "#model_data = data_new.drop(columns_to_drop, axis=1) # This data does not include Principle Component Analysis\n",
    "\n",
    "# Apply Principle Component to original data and drop columns\n",
    "model_data = data_new.drop(['PM2.5', 'PM10'], axis=1)\n",
    "# Add PC1\n",
    "model_data.insert(2, 'PC1', pm_pca[:, 0])\n",
    "model_data.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Date</th>\n",
       "      <th>PC1</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>AQI</th>\n",
       "      <th>humidity_mean</th>\n",
       "      <th>humidity_max</th>\n",
       "      <th>cloud_cover_sum</th>\n",
       "      <th>wind_speed_10m_mean</th>\n",
       "      <th>wind_speed_100m_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>0.823546</td>\n",
       "      <td>1.44</td>\n",
       "      <td>20.5</td>\n",
       "      <td>10.72</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.24</td>\n",
       "      <td>127.09</td>\n",
       "      <td>184.0</td>\n",
       "      <td>64.906533</td>\n",
       "      <td>89.346016</td>\n",
       "      <td>819.000009</td>\n",
       "      <td>9.180352</td>\n",
       "      <td>27.248455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City       Date       PC1    NO   NO2    NH3    CO    SO2      O3  \\\n",
       "0  Amaravati 2017-11-25  0.823546  1.44  20.5  10.72  0.12  15.24  127.09   \n",
       "\n",
       "     AQI  humidity_mean  humidity_max  cloud_cover_sum  wind_speed_10m_mean  \\\n",
       "0  184.0      64.906533     89.346016       819.000009             9.180352   \n",
       "\n",
       "   wind_speed_100m_max  \n",
       "0            27.248455  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark (Ordinary Least Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Mean Squared Error: 625.3157716943583\n",
      "OLS R-squared: 0.8341041695688785\n",
      "Cross-validated MSE: 964.3690845224761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the data\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r_squared = r2_score(y_test, y_pred)\n",
    "print(f\"OLS Mean Squared Error: {mse}\")\n",
    "print(f\"OLS R-squared: {r_squared}\")\n",
    "\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(linear_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "print(f\"Cross-validated MSE: {-cv_scores.mean()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor MSE: 652.1929303425775\n",
      "Random Forest Regressor R-squared: 0.826973678454775\n",
      "Cross-validated MSE (for each fold): [ 500.51920734 1312.04501078 1398.53029624  886.02399918  507.42781716]\n",
      "Average MSE: 920.9092661413919\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Regressor MSE: {mse_rf}\")\n",
    "print(f\"Random Forest Regressor R-squared: {r2_rf}\")\n",
    "\n",
    "rf_cv = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "                           \n",
    "# Output the results (MSE for each fold)\n",
    "print(\"Cross-validated MSE (for each fold):\", -rf_cv)\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -rf_cv.mean())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning using RandomisedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'max_depth': 19, 'max_features': None, 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 84}\n",
      "Tuned Random Forest MSE: 558.9869116127885\n",
      "Tuned Random Forest R-squared: 0.8517011690735703\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Prepare data\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),  # Random number of trees between 50 and 200\n",
    "    'max_depth': randint(5, 20),  # Random depth between 5 and 20\n",
    "    'min_samples_split': randint(2, 10),  # Random min samples for split between 2 and 10\n",
    "    'min_samples_leaf': randint(1, 10),  # Random min samples at leaf between 1 and 10\n",
    "    'max_features': ['sqrt', 'log2', None]  # Replace 'auto' with 'sqrt', 'log2', or None\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(rf, param_distributions=param_dist, \n",
    "                                   n_iter=20, cv=5, scoring='neg_mean_squared_error', \n",
    "                                   random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse_best_rf = mean_squared_error(y_test, y_pred_best_rf)\n",
    "r2_best_rf = r2_score(y_test, y_pred_best_rf)\n",
    "\n",
    "print(f\"Tuned Random Forest MSE: {mse_best_rf}\")\n",
    "print(f\"Tuned Random Forest R-squared: {r2_best_rf}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Regressor MSE: 1698.8290376151438\n",
      "Support Vector Regressor R-squared: 0.5493018620144134\n",
      "Cross-validated MSE (for each fold): [2933.57395727 5734.57493715 5187.31504924 4923.81426524 2142.00577555]\n",
      "Average MSE: 4184.2567968890335\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data (SVR requires standardized input)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the Support Vector Regression model\n",
    "svr = SVR(kernel='rbf')  # Using RBF kernel for non-linear relationships\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_svr = svr.predict(X_test_scaled)\n",
    "mse_svr = mean_squared_error(y_test, y_pred_svr)\n",
    "r2_svr = r2_score(y_test, y_pred_svr)\n",
    "\n",
    "print(f\"Support Vector Regressor MSE: {mse_svr}\")\n",
    "print(f\"Support Vector Regressor R-squared: {r2_svr}\")\n",
    "\n",
    "svr_cv = cross_val_score(svr, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "                           \n",
    "# Output the results (MSE for each fold)\n",
    "print(\"Cross-validated MSE (for each fold):\", -svr_cv)\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -svr_cv.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning using RandomisedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 5.908361216819946, 'epsilon': 0.8761761457749352, 'gamma': 'auto', 'kernel': 'linear'}\n",
      "Tuned SVR MSE: 631.858721247408\n",
      "Tuned SVR R-squared: 0.8323683296961194\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "# Prepare the data\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data (SVR requires standardized input)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Support Vector Regressor (SVR)\n",
    "svr = SVR()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'C': uniform(0.1, 100),  # Regularization parameter between 0.1 and 100\n",
    "    'epsilon': uniform(0.01, 1),  # Epsilon value for margin\n",
    "    'gamma': ['scale', 'auto'],  # Gamma parameter for the RBF kernel\n",
    "    'kernel': ['rbf', 'linear']  # Kernel options\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for tuning\n",
    "random_search_svr = RandomizedSearchCV(svr, param_distributions=param_dist, \n",
    "                                       n_iter=20, cv=5, scoring='neg_mean_squared_error', \n",
    "                                       random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search_svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_svr = random_search_svr.best_params_\n",
    "print(f\"Best parameters found: {best_params_svr}\")\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_svr = random_search_svr.best_estimator_\n",
    "y_pred_best_svr = best_svr.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse_best_svr = mean_squared_error(y_test, y_pred_best_svr)\n",
    "r2_best_svr = r2_score(y_test, y_pred_best_svr)\n",
    "\n",
    "print(f\"Tuned SVR MSE: {mse_best_svr}\")\n",
    "print(f\"Tuned SVR R-squared: {r2_best_svr}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MSE: 677.4291303918911\n",
      "XGBoost R-squared: 0.8202785324923394\n",
      "Cross-validated MSE (for each fold): [ 445.98349287 1040.89451183 1531.04799297 1025.71367514  483.09383664]\n",
      "Cross-validated R-squared (for each fold): [0.87358131 0.83300601 0.71099918 0.7348219  0.77481558]\n",
      "Average MSE: 905.3467018919616\n",
      "Average R-squared: 0.7854447955011884\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Prepare the data\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost Regressor with default or manually chosen parameters\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model's performance on the test set\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost MSE: {mse_xgb}\")\n",
    "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
    "\n",
    "# Perform cross-validation for MSE and R-squared\n",
    "xgb_cv_mse = cross_val_score(xgb_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "xgb_cv_r2 = cross_val_score(xgb_model, X, y, cv=5, scoring='r2')\n",
    "\n",
    "# Output the cross-validated results\n",
    "print(\"Cross-validated MSE (for each fold):\", -xgb_cv_mse)\n",
    "print(\"Cross-validated R-squared (for each fold):\", xgb_cv_r2)\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -xgb_cv_mse.mean())\n",
    "print(\"Average R-squared:\", xgb_cv_r2.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'colsample_bytree': 0.6974407590877849, 'learning_rate': 0.09804645241541143, 'max_depth': 9, 'n_estimators': 188, 'reg_alpha': 0.019884240408880517, 'reg_lambda': 0.81134195274865, 'subsample': 0.8950877702656028}\n",
      "Tuned XGBoost MSE: 647.2734038434045\n",
      "Tuned XGBoost R-squared: 0.828278825343517\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Prepare the data\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),  # Number of trees (estimators)\n",
    "    'learning_rate': uniform(0.01, 0.3),  # Learning rate (eta)\n",
    "    'max_depth': randint(3, 10),  # Depth of trees\n",
    "    'subsample': uniform(0.5, 0.5),  # Subsampling of rows\n",
    "    'colsample_bytree': uniform(0.5, 0.5),  # Subsampling of columns\n",
    "    'reg_alpha': uniform(0, 0.1),  # L1 regularization (alpha)\n",
    "    'reg_lambda': uniform(0.1, 1.0),  # L2 regularization (lambda)\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for tuning\n",
    "random_search_xgb = RandomizedSearchCV(xgb_model, param_distributions=param_dist, \n",
    "                                       n_iter=20, cv=5, scoring='neg_mean_squared_error', \n",
    "                                       random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_xgb = random_search_xgb.best_params_\n",
    "print(f\"Best parameters found: {best_params_xgb}\")\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_xgb = random_search_xgb.best_estimator_\n",
    "y_pred_best_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the best model\n",
    "mse_best_xgb = mean_squared_error(y_test, y_pred_best_xgb)\n",
    "r2_best_xgb = r2_score(y_test, y_pred_best_xgb)\n",
    "\n",
    "print(f\"Tuned XGBoost MSE: {mse_best_xgb}\")\n",
    "print(f\"Tuned XGBoost R-squared: {r2_best_xgb}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Neural Network + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Hybrid Model (XGBoost + Neural Network) MSE: 628.6942810508422\n",
      "Tuned Hybrid Model (XGBoost + Neural Network) R-squared: 0.8332078534344004\n",
      "Cross-validated MSE (for each fold): [ 495.08266179 1112.84092223 1385.81644434  888.43936225  478.45412347]\n",
      "Cross-validated r-squared (for each fold): [0.85966363 0.82146342 0.7384131  0.77031147 0.77697829]\n",
      "Average MSE: 872.1267028160009\n",
      "Average r-squared: 0.7933659847179328\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Initialize XGBoost and Neural Network as base learners\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "nn_model = MLPRegressor(hidden_layer_sizes=(50,), activation='relu', solver='adam', max_iter=5000, random_state=42)\n",
    "\n",
    "# Create the stacking model using Linear Regression as the meta-model\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=[('xgb', xgb_model), ('nn', nn_model)],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the stacked model\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred_stacked = stacked_model.predict(X_test)\n",
    "mse_stacked = mean_squared_error(y_test, y_pred_stacked)\n",
    "r2_stacked = r2_score(y_test, y_pred_stacked)\n",
    "\n",
    "print(f\"Tuned Hybrid Model (XGBoost + Neural Network) MSE: {mse_stacked}\")\n",
    "print(f\"Tuned Hybrid Model (XGBoost + Neural Network) R-squared: {r2_stacked}\")\n",
    "\n",
    "hybrid_cv = cross_val_score(stacked_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "hybrid_cv_r2 = cross_val_score(stacked_model, X, y, cv=5, scoring='r2')\n",
    "                           \n",
    "# Output the results (MSE for each fold)\n",
    "print(\"Cross-validated MSE (for each fold):\", -hybrid_cv)\n",
    "print(\"Cross-validated r-squared (for each fold):\", hybrid_cv_r2)\n",
    "\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -hybrid_cv.mean())\n",
    "print(\"Average r-squared:\", hybrid_cv_r2.mean())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning using RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'nn__activation': 'relu', 'nn__alpha': 0.002284404372168336, 'nn__hidden_layer_sizes': (100,), 'nn__solver': 'adam', 'xgb__colsample_bytree': 0.9416401294594341, 'xgb__learning_rate': 0.10730350630158218, 'xgb__max_depth': 9, 'xgb__n_estimators': 141, 'xgb__subsample': 0.6781489190384875}\n",
      "Tuned Stacked Model MSE: 649.3251918773118\n",
      "Tuned Stacked Model R-squared: 0.8277344874343173\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Prepare data\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize base models\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "nn_model = MLPRegressor(max_iter=5000, random_state=42, verbose=False)\n",
    "\n",
    "# Define the stacking model\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=[('xgb', xgb_model), ('nn', nn_model)],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': randint(50, 200),\n",
    "    'xgb__learning_rate': uniform(0.01, 0.3),\n",
    "    'xgb__max_depth': randint(3, 10),\n",
    "    'xgb__subsample': uniform(0.5, 0.5),  # Generates values strictly between 0.5 and 1.0\n",
    "    'xgb__colsample_bytree': uniform(0.5, 0.5),  # Same, strictly between 0.5 and 1.0\n",
    "    \n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (100, 100)],\n",
    "    'nn__activation': ['relu'],\n",
    "    'nn__alpha': uniform(0.0001, 0.01),  # Reasonable regularization range\n",
    "    'nn__solver': ['adam']\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=stacked_model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=20, \n",
    "                                   cv=5, \n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   random_state=42, \n",
    "                                   n_jobs=1,  # No parallel jobs\n",
    "                                   error_score='raise')  # Raise errors\n",
    "\n",
    "\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_stacked_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Make predictions and evaluate the tuned stacked model\n",
    "y_pred_best_stacked = best_stacked_model.predict(X_test)\n",
    "mse_best_stacked = mean_squared_error(y_test, y_pred_best_stacked)\n",
    "r2_best_stacked = r2_score(y_test, y_pred_best_stacked)\n",
    "\n",
    "print(f\"Tuned Stacked Model MSE: {mse_best_stacked}\")\n",
    "print(f\"Tuned Stacked Model R-squared: {r2_best_stacked}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 19.085882222567662\n",
      "P-value: 0.00018721094205127476\n",
      "There is a statistically significant difference between the models.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "mse_model1 = -rf_cv\n",
    "mse_model2 = -svr_cv\n",
    "mse_model3 = -hybrid_cv\n",
    "\n",
    "# Step 3: Apply ANOVA test to compare the models\n",
    "f_statistic, p_value = f_oneway(mse_model1, mse_model2, mse_model3)\n",
    "\n",
    "# Step 4: Output the results\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretation\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference between the models.\")\n",
    "else:\n",
    "    print(\"No statistically significant difference between the models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Multiple Comparison of Means - Tukey HSD, FWER=0.05           \n",
      "========================================================================\n",
      "    group1        group2     meandiff p-adj    lower      upper   reject\n",
      "------------------------------------------------------------------------\n",
      " Hybrid Model Random Forest   11.1165 0.9998 -1618.8031  1641.036  False\n",
      " Hybrid Model           SVG  3274.464 0.0005  1644.5445 4904.3835   True\n",
      "Random Forest           SVG 3263.3475 0.0005   1633.428 4893.2671   True\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Example MSE data for three models\n",
    "# Replace with your cross-validated MSE data\n",
    "mse_data = np.concatenate([mse_model1, mse_model2, mse_model3])\n",
    "\n",
    "# Corresponding labels for the models\n",
    "model_labels = (['Random Forest'] * len(mse_model1)) + (['SVG'] * len(mse_model2)) + (['Hybrid Model'] * len(mse_model3))\n",
    "\n",
    "# Create a DataFrame to hold the MSE values and corresponding model labels\n",
    "df = pd.DataFrame({'MSE': mse_data, 'Model': model_labels})\n",
    "\n",
    "# Perform Tukey's HSD test\n",
    "tukey = pairwise_tukeyhsd(endog=df['MSE'], groups=df['Model'], alpha=0.05)\n",
    "\n",
    "# Display the test results\n",
    "print(tukey)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
