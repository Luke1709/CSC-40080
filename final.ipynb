{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Iterative Imputer & Random Forest\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('city_day.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Date</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>Benzene</th>\n",
       "      <th>Toluene</th>\n",
       "      <th>Xylene</th>\n",
       "      <th>AQI</th>\n",
       "      <th>AQI_Bucket</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>18.22</td>\n",
       "      <td>17.15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.92</td>\n",
       "      <td>27.64</td>\n",
       "      <td>133.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>15.69</td>\n",
       "      <td>16.46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.97</td>\n",
       "      <td>24.55</td>\n",
       "      <td>34.06</td>\n",
       "      <td>3.68</td>\n",
       "      <td>5.50</td>\n",
       "      <td>3.77</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-03</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.40</td>\n",
       "      <td>19.30</td>\n",
       "      <td>29.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.40</td>\n",
       "      <td>29.07</td>\n",
       "      <td>30.70</td>\n",
       "      <td>6.80</td>\n",
       "      <td>16.40</td>\n",
       "      <td>2.25</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.70</td>\n",
       "      <td>18.48</td>\n",
       "      <td>17.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.70</td>\n",
       "      <td>18.59</td>\n",
       "      <td>36.08</td>\n",
       "      <td>4.43</td>\n",
       "      <td>10.14</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ahmedabad</td>\n",
       "      <td>2015-01-05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.10</td>\n",
       "      <td>21.42</td>\n",
       "      <td>37.76</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.10</td>\n",
       "      <td>39.33</td>\n",
       "      <td>39.31</td>\n",
       "      <td>7.01</td>\n",
       "      <td>18.89</td>\n",
       "      <td>2.78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City        Date  PM2.5  PM10     NO    NO2    NOx  NH3     CO    SO2  \\\n",
       "0  Ahmedabad  2015-01-01    NaN   NaN   0.92  18.22  17.15  NaN   0.92  27.64   \n",
       "1  Ahmedabad  2015-01-02    NaN   NaN   0.97  15.69  16.46  NaN   0.97  24.55   \n",
       "2  Ahmedabad  2015-01-03    NaN   NaN  17.40  19.30  29.70  NaN  17.40  29.07   \n",
       "3  Ahmedabad  2015-01-04    NaN   NaN   1.70  18.48  17.97  NaN   1.70  18.59   \n",
       "4  Ahmedabad  2015-01-05    NaN   NaN  22.10  21.42  37.76  NaN  22.10  39.33   \n",
       "\n",
       "       O3  Benzene  Toluene  Xylene  AQI AQI_Bucket  \n",
       "0  133.36     0.00     0.02    0.00  NaN        NaN  \n",
       "1   34.06     3.68     5.50    3.77  NaN        NaN  \n",
       "2   30.70     6.80    16.40    2.25  NaN        NaN  \n",
       "3   36.08     4.43    10.14    1.00  NaN        NaN  \n",
       "4   39.31     7.01    18.89    2.78  NaN        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Drop AQI_Bucket, not needed for this task\n",
    "- Drop any rows missing AQI values from simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dataset from raw data, dropping AQI Bucket\n",
    "data = raw_data.drop(['AQI_Bucket'], axis=1)\n",
    "\n",
    "# Dropping rows with missing AQI values\n",
    "data = data.dropna(subset=['AQI'])\n",
    "\n",
    "# Convert the date to correct format\n",
    "data['Date'] = pd.to_datetime(data['Date'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce the data to 3 cities to reduce geographical variation: Jaipur, Amritsar, Thiruvananthapuram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['Amritsar', 'Amaravati', 'Jaipur']\n",
    "data = data[data['City'].isin(regions)]\n",
    "\n",
    "# Reset Index\n",
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>Benzene</th>\n",
       "      <th>Toluene</th>\n",
       "      <th>Xylene</th>\n",
       "      <th>AQI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3061</td>\n",
       "      <td>2971.000</td>\n",
       "      <td>3031.000</td>\n",
       "      <td>3006.000</td>\n",
       "      <td>3049.000</td>\n",
       "      <td>2727.000</td>\n",
       "      <td>3045.000</td>\n",
       "      <td>2979.000</td>\n",
       "      <td>2921.000</td>\n",
       "      <td>2976.000</td>\n",
       "      <td>2872.000</td>\n",
       "      <td>2855.000</td>\n",
       "      <td>1617.000</td>\n",
       "      <td>3061.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2019-01-04 22:02:23.482522112</td>\n",
       "      <td>50.159</td>\n",
       "      <td>107.412</td>\n",
       "      <td>13.281</td>\n",
       "      <td>24.375</td>\n",
       "      <td>30.779</td>\n",
       "      <td>17.991</td>\n",
       "      <td>0.665</td>\n",
       "      <td>11.003</td>\n",
       "      <td>35.744</td>\n",
       "      <td>2.194</td>\n",
       "      <td>4.145</td>\n",
       "      <td>4.885</td>\n",
       "      <td>118.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2017-02-28 00:00:00</td>\n",
       "      <td>2.850</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.860</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2018-04-13 00:00:00</td>\n",
       "      <td>28.375</td>\n",
       "      <td>62.905</td>\n",
       "      <td>4.880</td>\n",
       "      <td>11.640</td>\n",
       "      <td>15.945</td>\n",
       "      <td>9.720</td>\n",
       "      <td>0.440</td>\n",
       "      <td>7.210</td>\n",
       "      <td>21.210</td>\n",
       "      <td>0.280</td>\n",
       "      <td>1.100</td>\n",
       "      <td>0.150</td>\n",
       "      <td>74.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2019-01-16 00:00:00</td>\n",
       "      <td>43.630</td>\n",
       "      <td>97.700</td>\n",
       "      <td>10.400</td>\n",
       "      <td>18.960</td>\n",
       "      <td>26.780</td>\n",
       "      <td>14.270</td>\n",
       "      <td>0.660</td>\n",
       "      <td>10.250</td>\n",
       "      <td>31.420</td>\n",
       "      <td>1.130</td>\n",
       "      <td>2.240</td>\n",
       "      <td>2.000</td>\n",
       "      <td>104.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2019-10-17 00:00:00</td>\n",
       "      <td>63.585</td>\n",
       "      <td>138.735</td>\n",
       "      <td>16.555</td>\n",
       "      <td>32.030</td>\n",
       "      <td>39.130</td>\n",
       "      <td>23.070</td>\n",
       "      <td>0.860</td>\n",
       "      <td>13.250</td>\n",
       "      <td>46.240</td>\n",
       "      <td>2.850</td>\n",
       "      <td>5.515</td>\n",
       "      <td>8.130</td>\n",
       "      <td>143.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2020-07-01 00:00:00</td>\n",
       "      <td>868.660</td>\n",
       "      <td>917.080</td>\n",
       "      <td>103.440</td>\n",
       "      <td>237.270</td>\n",
       "      <td>150.960</td>\n",
       "      <td>129.460</td>\n",
       "      <td>3.830</td>\n",
       "      <td>67.260</td>\n",
       "      <td>172.280</td>\n",
       "      <td>53.890</td>\n",
       "      <td>76.320</td>\n",
       "      <td>137.450</td>\n",
       "      <td>869.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>35.894</td>\n",
       "      <td>62.526</td>\n",
       "      <td>12.981</td>\n",
       "      <td>17.878</td>\n",
       "      <td>22.030</td>\n",
       "      <td>12.889</td>\n",
       "      <td>0.440</td>\n",
       "      <td>5.941</td>\n",
       "      <td>20.465</td>\n",
       "      <td>3.328</td>\n",
       "      <td>5.079</td>\n",
       "      <td>8.077</td>\n",
       "      <td>65.766</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Date     PM2.5      PM10        NO       NO2  \\\n",
       "count                           3061  2971.000  3031.000  3006.000  3049.000   \n",
       "mean   2019-01-04 22:02:23.482522112    50.159   107.412    13.281    24.375   \n",
       "min              2017-02-28 00:00:00     2.850     0.420     0.250     0.010   \n",
       "25%              2018-04-13 00:00:00    28.375    62.905     4.880    11.640   \n",
       "50%              2019-01-16 00:00:00    43.630    97.700    10.400    18.960   \n",
       "75%              2019-10-17 00:00:00    63.585   138.735    16.555    32.030   \n",
       "max              2020-07-01 00:00:00   868.660   917.080   103.440   237.270   \n",
       "std                              NaN    35.894    62.526    12.981    17.878   \n",
       "\n",
       "            NOx       NH3        CO       SO2        O3   Benzene   Toluene  \\\n",
       "count  2727.000  3045.000  2979.000  2921.000  2976.000  2872.000  2855.000   \n",
       "mean     30.779    17.991     0.665    11.003    35.744     2.194     4.145   \n",
       "min       0.860     0.060     0.000     0.710     0.240     0.000     0.000   \n",
       "25%      15.945     9.720     0.440     7.210    21.210     0.280     1.100   \n",
       "50%      26.780    14.270     0.660    10.250    31.420     1.130     2.240   \n",
       "75%      39.130    23.070     0.860    13.250    46.240     2.850     5.515   \n",
       "max     150.960   129.460     3.830    67.260   172.280    53.890    76.320   \n",
       "std      22.030    12.889     0.440     5.941    20.465     3.328     5.079   \n",
       "\n",
       "         Xylene       AQI  \n",
       "count  1617.000  3061.000  \n",
       "mean      4.885   118.074  \n",
       "min       0.000    20.000  \n",
       "25%       0.150    74.000  \n",
       "50%       2.000   104.000  \n",
       "75%       8.130   143.000  \n",
       "max     137.450   869.000  \n",
       "std       8.077    65.766  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data summaries \n",
    "np.round(data.describe(),3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "City          0\n",
       "Date          0\n",
       "PM2.5        90\n",
       "PM10         30\n",
       "NO           55\n",
       "NO2          12\n",
       "NOx         334\n",
       "NH3          16\n",
       "CO           82\n",
       "SO2         140\n",
       "O3           85\n",
       "Benzene     189\n",
       "Toluene     206\n",
       "Xylene     1444\n",
       "AQI           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As per EDA, drop Benzene, Toluene, Xylene as these do not directly correlate with AQI and have significant numbers of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Benzene, Toluene, Xylene from data\n",
    "data.drop(['Benzene', 'Toluene', 'Xylene'], axis=1, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add missing Values using Iterative Imputer package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PM2.5    0\n",
      "PM10     0\n",
      "NO       0\n",
      "NO2      0\n",
      "NOx      0\n",
      "NH3      0\n",
      "CO       0\n",
      "SO2      0\n",
      "O3       0\n",
      "AQI      0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukee\\anaconda3\\Lib\\site-packages\\sklearn\\impute\\_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Drop the 'City' and 'Date' column, as it's not used for imputation\n",
    "data_model = data.drop(['City', 'Date'], axis=1)\n",
    "\n",
    "# Initialize the Iterative Imputer with a RandomForestRegressor\n",
    "imputer = IterativeImputer(estimator=RandomForestRegressor(), max_iter=10, random_state=42)\n",
    "\n",
    "# Apply the imputer to the dataset\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(data_model), columns=data_model.columns)\n",
    "\n",
    "# Display information about missing values after imputation\n",
    "print(df_imputed.isnull().sum())\n",
    "\n",
    "# Merge the imputed dataframe and the dataset\n",
    "data_new = pd.concat([data[['City', 'Date']], df_imputed], axis=1)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Datasets\n",
    "amarvati = pd.read_csv('amarvati.csv')\n",
    "amritsar = pd.read_csv('amritsar.csv')\n",
    "jaipur = pd.read_csv('jaipur.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'Date' columns to datetime and remove timezones for consistency\n",
    "data_new['Date'] = pd.to_datetime(data_new['Date']).dt.tz_localize(None)\n",
    "amarvati['date'] = pd.to_datetime(amarvati['date']).dt.tz_localize(None)\n",
    "jaipur['date'] = pd.to_datetime(jaipur['date']).dt.tz_localize(None)\n",
    "amritsar['date'] = pd.to_datetime(amritsar['date']).dt.tz_localize(None)\n",
    "\n",
    "# Rename the 'date' columns to 'Date' for consistency\n",
    "amarvati.rename(columns={'date': 'Date'}, inplace=True)\n",
    "jaipur.rename(columns={'date': 'Date'}, inplace=True)\n",
    "amritsar.rename(columns={'date': 'Date'}, inplace=True)\n",
    "\n",
    "# Add the 'City' column to each weather dataset\n",
    "amarvati['City'] = 'Amaravati'\n",
    "jaipur['City'] = 'Jaipur'\n",
    "amritsar['City'] = 'Amritsar'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Date</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NOx</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>...</th>\n",
       "      <th>humidity_max</th>\n",
       "      <th>dew_point_mean</th>\n",
       "      <th>dew_point_max</th>\n",
       "      <th>precipitation_sum</th>\n",
       "      <th>rain_sum</th>\n",
       "      <th>cloud_cover_sum</th>\n",
       "      <th>wind_speed_10m_mean</th>\n",
       "      <th>wind_speed_10m_max</th>\n",
       "      <th>wind_speed_100m_mean</th>\n",
       "      <th>wind_speed_100m_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>81.40</td>\n",
       "      <td>124.50</td>\n",
       "      <td>1.44</td>\n",
       "      <td>20.50</td>\n",
       "      <td>12.08</td>\n",
       "      <td>10.72</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.24</td>\n",
       "      <td>...</td>\n",
       "      <td>89.346016</td>\n",
       "      <td>19.013000</td>\n",
       "      <td>21.263000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>819.000009</td>\n",
       "      <td>9.180352</td>\n",
       "      <td>16.808570</td>\n",
       "      <td>15.935398</td>\n",
       "      <td>27.248455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-26</td>\n",
       "      <td>78.32</td>\n",
       "      <td>129.06</td>\n",
       "      <td>1.26</td>\n",
       "      <td>26.00</td>\n",
       "      <td>14.85</td>\n",
       "      <td>10.28</td>\n",
       "      <td>0.14</td>\n",
       "      <td>26.96</td>\n",
       "      <td>...</td>\n",
       "      <td>95.198590</td>\n",
       "      <td>18.248417</td>\n",
       "      <td>21.063000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>532.800023</td>\n",
       "      <td>7.759277</td>\n",
       "      <td>11.988594</td>\n",
       "      <td>14.097954</td>\n",
       "      <td>22.183128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-27</td>\n",
       "      <td>88.76</td>\n",
       "      <td>135.32</td>\n",
       "      <td>6.60</td>\n",
       "      <td>30.85</td>\n",
       "      <td>21.77</td>\n",
       "      <td>12.91</td>\n",
       "      <td>0.11</td>\n",
       "      <td>33.59</td>\n",
       "      <td>...</td>\n",
       "      <td>97.275760</td>\n",
       "      <td>18.960917</td>\n",
       "      <td>21.463001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>618.900014</td>\n",
       "      <td>6.287076</td>\n",
       "      <td>9.085988</td>\n",
       "      <td>11.361810</td>\n",
       "      <td>17.418196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>64.18</td>\n",
       "      <td>104.09</td>\n",
       "      <td>2.56</td>\n",
       "      <td>28.07</td>\n",
       "      <td>17.01</td>\n",
       "      <td>11.42</td>\n",
       "      <td>0.09</td>\n",
       "      <td>19.00</td>\n",
       "      <td>...</td>\n",
       "      <td>94.871315</td>\n",
       "      <td>17.425500</td>\n",
       "      <td>21.013000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>378.000013</td>\n",
       "      <td>7.019016</td>\n",
       "      <td>13.684735</td>\n",
       "      <td>13.233190</td>\n",
       "      <td>25.630886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-29</td>\n",
       "      <td>72.47</td>\n",
       "      <td>114.84</td>\n",
       "      <td>5.23</td>\n",
       "      <td>23.20</td>\n",
       "      <td>16.59</td>\n",
       "      <td>12.25</td>\n",
       "      <td>0.16</td>\n",
       "      <td>10.55</td>\n",
       "      <td>...</td>\n",
       "      <td>96.376900</td>\n",
       "      <td>17.617167</td>\n",
       "      <td>20.663000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.000007</td>\n",
       "      <td>10.355873</td>\n",
       "      <td>14.186923</td>\n",
       "      <td>18.022025</td>\n",
       "      <td>23.904108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        City       Date  PM2.5    PM10    NO    NO2    NOx    NH3    CO  \\\n",
       "0  Amaravati 2017-11-25  81.40  124.50  1.44  20.50  12.08  10.72  0.12   \n",
       "1  Amaravati 2017-11-26  78.32  129.06  1.26  26.00  14.85  10.28  0.14   \n",
       "2  Amaravati 2017-11-27  88.76  135.32  6.60  30.85  21.77  12.91  0.11   \n",
       "3  Amaravati 2017-11-28  64.18  104.09  2.56  28.07  17.01  11.42  0.09   \n",
       "4  Amaravati 2017-11-29  72.47  114.84  5.23  23.20  16.59  12.25  0.16   \n",
       "\n",
       "     SO2  ...  humidity_max  dew_point_mean  dew_point_max  precipitation_sum  \\\n",
       "0  15.24  ...     89.346016       19.013000      21.263000                0.0   \n",
       "1  26.96  ...     95.198590       18.248417      21.063000                0.0   \n",
       "2  33.59  ...     97.275760       18.960917      21.463001                0.0   \n",
       "3  19.00  ...     94.871315       17.425500      21.013000                0.0   \n",
       "4  10.55  ...     96.376900       17.617167      20.663000                0.0   \n",
       "\n",
       "   rain_sum  cloud_cover_sum  wind_speed_10m_mean  wind_speed_10m_max  \\\n",
       "0       0.0       819.000009             9.180352           16.808570   \n",
       "1       0.0       532.800023             7.759277           11.988594   \n",
       "2       0.0       618.900014             6.287076            9.085988   \n",
       "3       0.0       378.000013             7.019016           13.684735   \n",
       "4       0.0       189.000007            10.355873           14.186923   \n",
       "\n",
       "   wind_speed_100m_mean  wind_speed_100m_max  \n",
       "0             15.935398            27.248455  \n",
       "1             14.097954            22.183128  \n",
       "2             11.361810            17.418196  \n",
       "3             13.233190            25.630886  \n",
       "4             18.022025            23.904108  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to map cities to the correct city csv file\n",
    "city_data = {\n",
    "    'Amaravati': amarvati,\n",
    "    'Jaipur': jaipur,\n",
    "    'Amritsar': amritsar\n",
    "}\n",
    "\n",
    "# Initialise empty lists for storing new data\n",
    "temperature_mean = []\n",
    "temperature_max = []\n",
    "humidity_mean = []\n",
    "humidity_max = []\n",
    "dew_point_mean = []\n",
    "dew_point_max = []\n",
    "precipitation_sum = []\n",
    "rain_sum = []\n",
    "cloud_cover_sum = []\n",
    "wind_speed_10m_mean = []\n",
    "wind_speed_10m_max = []\n",
    "wind_speed_100m_mean = []\n",
    "wind_speed_100m_max = []\n",
    "\n",
    "# Loop over each row in data_new_df\n",
    "for index, row in data_new.iterrows():\n",
    "    city = row['City']\n",
    "    date = row['Date']\n",
    "    \n",
    "    # Check if the city has corresponding weather data\n",
    "    if city in city_data:\n",
    "        weather_df = city_data[city]\n",
    "        # Find the matching row in the city's weather DataFrame\n",
    "        match = weather_df[weather_df['Date'] == date]\n",
    "        \n",
    "        if not match.empty:\n",
    "            # Append values to the lists if a match is found\n",
    "            temperature_mean.append(match['temperature_mean'].values[0])\n",
    "            temperature_max.append(match['temperature_max'].values[0])\n",
    "            humidity_mean.append(match['humidity_mean'].values[0])\n",
    "            humidity_max.append(match['humidity_max'].values[0])\n",
    "            dew_point_mean.append(match['dew_point_mean'].values[0])\n",
    "            dew_point_max.append(match['dew_point_max'].values[0])\n",
    "            precipitation_sum.append(match['precipitation_sum'].values[0])\n",
    "            rain_sum.append(match['rain_sum'].values[0])\n",
    "            cloud_cover_sum.append(match['cloud_cover_sum'].values[0])\n",
    "            wind_speed_10m_mean.append(match['wind_speed_10m_mean'].values[0])\n",
    "            wind_speed_10m_max.append(match['wind_speed_10m_max'].values[0])\n",
    "            wind_speed_100m_mean.append(match['wind_speed_100m_mean'].values[0])\n",
    "            wind_speed_100m_max.append(match['wind_speed_100m_max'].values[0])\n",
    "        else:\n",
    "            # Append NaN if no match is found\n",
    "            temperature_mean.append(float('nan'))\n",
    "            temperature_max.append(float('nan'))\n",
    "            humidity_mean.append(float('nan'))\n",
    "            humidity_max.append(float('nan'))\n",
    "            dew_point_mean.append(float('nan'))\n",
    "            dew_point_max.append(float('nan'))\n",
    "            precipitation_sum.append(float('nan'))\n",
    "            rain_sum.append(float('nan'))\n",
    "            cloud_cover_sum.append(float('nan'))\n",
    "            wind_speed_10m_mean.append(float('nan'))\n",
    "            wind_speed_10m_max.append(float('nan'))\n",
    "            wind_speed_100m_mean.append(float('nan'))\n",
    "            wind_speed_100m_max.append(float('nan'))\n",
    "    else:\n",
    "        # Append NaN if no matching city is found\n",
    "        temperature_mean.append(float('nan'))\n",
    "        temperature_max.append(float('nan'))\n",
    "        humidity_mean.append(float('nan'))\n",
    "        humidity_max.append(float('nan'))\n",
    "        dew_point_mean.append(float('nan'))\n",
    "        dew_point_max.append(float('nan'))\n",
    "        precipitation_sum.append(float('nan'))\n",
    "        rain_sum.append(float('nan'))\n",
    "        cloud_cover_sum.append(float('nan'))\n",
    "        wind_speed_10m_mean.append(float('nan'))\n",
    "        wind_speed_10m_max.append(float('nan'))\n",
    "        wind_speed_100m_mean.append(float('nan'))\n",
    "        wind_speed_100m_max.append(float('nan'))\n",
    "\n",
    "# Add the new data to the DataFrame\n",
    "data_new['temperature_mean'] = temperature_mean\n",
    "data_new['temperature_max'] = temperature_max\n",
    "data_new['humidity_mean'] = humidity_mean\n",
    "data_new['humidity_max'] = humidity_max\n",
    "data_new['dew_point_mean'] = dew_point_mean\n",
    "data_new['dew_point_max'] = dew_point_max\n",
    "data_new['precipitation_sum'] = precipitation_sum\n",
    "data_new['rain_sum'] = rain_sum\n",
    "data_new['cloud_cover_sum'] = cloud_cover_sum\n",
    "data_new['wind_speed_10m_mean'] = wind_speed_10m_mean\n",
    "data_new['wind_speed_10m_max'] = wind_speed_10m_max\n",
    "data_new['wind_speed_100m_mean'] = wind_speed_100m_mean\n",
    "data_new['wind_speed_100m_max'] = wind_speed_100m_max\n",
    "\n",
    "data_new.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Ridge & Lasso Regression to examine features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features and target\n",
    "X = data_new[['PM2.5', 'PM10', 'NO', 'NO2', 'NOx', 'NH3', 'CO', 'SO2', 'O3', 'temperature_mean', 'temperature_max', 'humidity_mean', 'humidity_max', 'dew_point_mean', 'dew_point_max', 'precipitation_sum', 'rain_sum', 'cloud_cover_sum', 'wind_speed_10m_mean', 'wind_speed_10m_max', 'wind_speed_100m_mean', 'wind_speed_100m_max']]\n",
    "y = data_new['AQI'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 712.9771021871375\n",
      "R-squared: 0.8108476808041831\n",
      "                 Feature  Coefficient\n",
      "0                  PM2.5      18.8946\n",
      "1                   PM10      42.8097\n",
      "2                     NO       3.5840\n",
      "3                    NO2       4.8146\n",
      "4                    NOx      -2.9495\n",
      "5                    NH3      -1.9682\n",
      "6                     CO       3.3890\n",
      "7                    SO2       0.0261\n",
      "8                     O3       6.3516\n",
      "9       temperature_mean       0.7014\n",
      "10       temperature_max       0.0850\n",
      "11         humidity_mean       2.1310\n",
      "12          humidity_max       1.9543\n",
      "13        dew_point_mean       3.8505\n",
      "14         dew_point_max      -5.6574\n",
      "15     precipitation_sum      -0.1968\n",
      "16              rain_sum      -0.1968\n",
      "17       cloud_cover_sum       0.3904\n",
      "18   wind_speed_10m_mean       2.0555\n",
      "19    wind_speed_10m_max      -1.9380\n",
      "20  wind_speed_100m_mean       0.1900\n",
      "21   wind_speed_100m_max       1.5478\n",
      "Cross-validated MSE (for each fold): [ 790.53724162 2132.16251967 1079.81864377  775.22866485  621.4122969 ]\n",
      "Average MSE: 1079.8318733633316\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardise both sets of data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initiate Ridge Regression\n",
    "ridge = Ridge(alpha=1) \n",
    "\n",
    "# Fit the model\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(ridge.coef_,4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "ridge_cv = cross_val_score(ridge, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "                           \n",
    "# Output the results (MSE for each fold)\n",
    "print(\"Cross-validated MSE (for each fold):\", -ridge_cv)\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -ridge_cv.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Ridge: 100\n",
      "Mean Squared Error: 705.6397672648468\n",
      "R-squared: 0.8127942705516101\n",
      "                 Feature  Coefficient\n",
      "0                  PM2.5      19.4086\n",
      "1                   PM10      40.2720\n",
      "2                     NO       2.6709\n",
      "3                    NO2       4.0080\n",
      "4                    NOx      -0.8548\n",
      "5                    NH3      -1.7024\n",
      "6                     CO       3.5029\n",
      "7                    SO2       0.0214\n",
      "8                     O3       6.3468\n",
      "9       temperature_mean      -0.1313\n",
      "10       temperature_max      -0.2759\n",
      "11         humidity_mean       1.4868\n",
      "12          humidity_max       1.7477\n",
      "13        dew_point_mean       1.5674\n",
      "14         dew_point_max      -2.2962\n",
      "15     precipitation_sum      -0.1408\n",
      "16              rain_sum      -0.1408\n",
      "17       cloud_cover_sum       0.1045\n",
      "18   wind_speed_10m_mean       1.2136\n",
      "19    wind_speed_10m_max      -1.1762\n",
      "20  wind_speed_100m_mean       0.6642\n",
      "21   wind_speed_100m_max       0.9243\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the alpha values to test for tuning\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Initiate Ridge Regression and GridSearchCV\n",
    "ridge = Ridge()\n",
    "\n",
    "# Use GridSearchCV to tune alpha\n",
    "ridge_cv = GridSearchCV(ridge, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best alpha value\n",
    "best_alpha = ridge_cv.best_params_['alpha']\n",
    "print(f\"Best alpha for Ridge: {best_alpha}\")\n",
    "\n",
    "# Use the best alpha to fit the model again\n",
    "ridge_best = Ridge(alpha=best_alpha)\n",
    "ridge_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge_best.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge_best.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(ridge_best.coef_, 4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 710.6474465898002\n",
      "R-squared: 0.8114657367807525\n",
      "                 Feature  Coefficient\n",
      "0                  PM2.5      18.8868\n",
      "1                   PM10      42.7105\n",
      "2                     NO       2.5207\n",
      "3                    NO2       3.9068\n",
      "4                    NOx      -1.2811\n",
      "5                    NH3      -1.7772\n",
      "6                     CO       3.2403\n",
      "7                    SO2       0.0763\n",
      "8                     O3       6.2857\n",
      "9       temperature_mean      -0.0000\n",
      "10       temperature_max      -0.0000\n",
      "11         humidity_mean       2.3126\n",
      "12          humidity_max       1.4390\n",
      "13        dew_point_mean       0.0652\n",
      "14         dew_point_max      -1.2261\n",
      "15     precipitation_sum      -0.1752\n",
      "16              rain_sum      -0.0000\n",
      "17       cloud_cover_sum       0.2030\n",
      "18   wind_speed_10m_mean       0.4384\n",
      "19    wind_speed_10m_max      -0.3285\n",
      "20  wind_speed_100m_mean       1.5320\n",
      "21   wind_speed_100m_max       0.0720\n",
      "Cross-validated MSE (for each fold): [ 786.54740847 2135.3795451  1078.3420597   772.28139465  615.32714956]\n",
      "Average MSE: 1077.5755114978165\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Using majority of the same code as above:\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = lasso.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(lasso.coef_,4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "lasso_cv = cross_val_score(lasso, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "                           \n",
    "# Output the results (MSE for each fold)\n",
    "print(\"Cross-validated MSE (for each fold):\", -lasso_cv)\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -lasso_cv.mean())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alpha Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Lasso: 1\n",
      "Mean Squared Error: 706.2392527069372\n",
      "R-squared: 0.8127942705516101\n",
      "                 Feature  Coefficient\n",
      "0                  PM2.5      18.1983\n",
      "1                   PM10      42.3546\n",
      "2                     NO       0.5230\n",
      "3                    NO2       2.4921\n",
      "4                    NOx       0.0000\n",
      "5                    NH3      -0.0000\n",
      "6                     CO       2.7493\n",
      "7                    SO2       0.0000\n",
      "8                     O3       4.9989\n",
      "9       temperature_mean      -0.0000\n",
      "10       temperature_max      -1.0879\n",
      "11         humidity_mean       0.0000\n",
      "12          humidity_max       1.2673\n",
      "13        dew_point_mean      -0.0000\n",
      "14         dew_point_max      -0.0000\n",
      "15     precipitation_sum       0.0000\n",
      "16              rain_sum       0.0000\n",
      "17       cloud_cover_sum       0.0000\n",
      "18   wind_speed_10m_mean       0.0000\n",
      "19    wind_speed_10m_max       0.0000\n",
      "20  wind_speed_100m_mean       0.2494\n",
      "21   wind_speed_100m_max       0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the alpha values to test for tuning\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Initiate Ridge Regression and GridSearchCV\n",
    "lasso = Lasso()\n",
    "\n",
    "# Use GridSearchCV to tune alpha\n",
    "lasso_cv = GridSearchCV(lasso, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best alpha value\n",
    "best_alpha = lasso_cv.best_params_['alpha']\n",
    "print(f\"Best alpha for Lasso: {best_alpha}\")\n",
    "\n",
    "# Use the best alpha to fit the model again\n",
    "lasso_best = Lasso(alpha=best_alpha)\n",
    "lasso_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lasso_best.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge_best.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(lasso_best.coef_, 4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As PM2.5 particles are included in PM10, we face the issue of multicolinearity. By applying PCA onto these features we can reduce this.\n",
    "\n",
    "- This improves the Ridge and Lasso Regression models but will sacrifice interpretability"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principle Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.85239276 0.14760724]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lukee\\AppData\\Local\\Temp\\ipykernel_18424\\408682948.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.drop(['PM2.5', 'PM10'], axis=1, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "pm_data_scaled = scaler.fit_transform(data_new[['PM2.5', 'PM10']])\n",
    "\n",
    "# Step 2: Apply PCA to PM2.5 and PM10\n",
    "pca = PCA(n_components=2)  # Use 2 components because we have 2 features\n",
    "pm_pca = pca.fit_transform(pm_data_scaled)\n",
    "\n",
    "# Create a new DataFrame with the principal components\n",
    "pm_pca_df = pd.DataFrame(pm_pca, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Explained variance to understand how much information is captured by each component\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "print(f\"Explained Variance Ratio: {explained_variance}\")\n",
    "\n",
    "# Keep PC1 and drop both PM2.5 and PM10\n",
    "X.drop(['PM2.5', 'PM10'], axis=1, inplace=True)\n",
    "\n",
    "# Add PC1\n",
    "X.insert(0, 'PC1', pm_pca[:, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 624.3095505364181\n",
      "R-squared: 0.8343711193279442\n",
      "                 Feature  Coefficient\n",
      "0                    PC1      55.7779\n",
      "1                     NO       3.5407\n",
      "2                    NO2       4.7551\n",
      "3                    NOx      -1.6064\n",
      "4                    NH3      -2.4049\n",
      "5                     CO       4.6070\n",
      "6                    SO2      -1.3819\n",
      "7                     O3       7.0875\n",
      "8       temperature_mean      -5.5693\n",
      "9        temperature_max       0.7448\n",
      "10         humidity_mean      -1.6504\n",
      "11          humidity_max       1.3888\n",
      "12        dew_point_mean       8.5312\n",
      "13         dew_point_max      -3.2471\n",
      "14     precipitation_sum       0.0799\n",
      "15              rain_sum       0.0799\n",
      "16       cloud_cover_sum      -0.8201\n",
      "17   wind_speed_10m_mean       2.6527\n",
      "18    wind_speed_10m_max      -1.6698\n",
      "19  wind_speed_100m_mean      -0.9926\n",
      "20   wind_speed_100m_max       2.4340\n",
      "Cross-validated MSE (for each fold): [ 669.97145786 1822.57050469 1184.70573794  706.48631154  531.08078623]\n",
      "Average MSE: 982.9629596495242\n"
     ]
    }
   ],
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardise both sets of data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initiate Ridge Regression\n",
    "ridge = Ridge(alpha=1.0) \n",
    "\n",
    "# Fit the model\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(ridge.coef_,4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "ridge_cv = cross_val_score(ridge, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "                           \n",
    "# Output the results (MSE for each fold)\n",
    "print(\"Cross-validated MSE (for each fold):\", -ridge_cv)\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -ridge_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Ridge: 10\n",
      "Mean Squared Error: 625.0387552469834\n",
      "R-squared: 0.8341776618357627\n",
      "                 Feature  Coefficient\n",
      "0                    PC1      55.4955\n",
      "1                     NO       3.4656\n",
      "2                    NO2       4.6762\n",
      "3                    NOx      -1.3687\n",
      "4                    NH3      -2.3773\n",
      "5                     CO       4.6130\n",
      "6                    SO2      -1.3613\n",
      "7                     O3       7.1015\n",
      "8       temperature_mean      -3.0909\n",
      "9        temperature_max       0.0711\n",
      "10         humidity_mean      -0.3259\n",
      "11          humidity_max       1.4769\n",
      "12        dew_point_mean       6.1643\n",
      "13         dew_point_max      -2.8677\n",
      "14     precipitation_sum       0.0550\n",
      "15              rain_sum       0.0550\n",
      "16       cloud_cover_sum      -0.9732\n",
      "17   wind_speed_10m_mean       2.4907\n",
      "18    wind_speed_10m_max      -1.5219\n",
      "19  wind_speed_100m_mean      -0.8256\n",
      "20   wind_speed_100m_max       2.2937\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the alpha values to test for tuning\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Initiate Ridge Regression and GridSearchCV\n",
    "ridge = Ridge()\n",
    "\n",
    "# Use GridSearchCV to tune alpha\n",
    "ridge_cv = GridSearchCV(ridge, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "ridge_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best alpha value\n",
    "best_alpha = ridge_cv.best_params_['alpha']\n",
    "print(f\"Best alpha for Ridge: {best_alpha}\")\n",
    "\n",
    "# Use the best alpha to fit the model again\n",
    "ridge_best = Ridge(alpha=best_alpha)\n",
    "ridge_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = ridge_best.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge_best.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(ridge_best.coef_, 4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 621.5314242454493\n",
      "R-squared: 0.8351081542612452\n",
      "                 Feature  Coefficient\n",
      "0                    PC1      55.7205\n",
      "1                     NO       2.6738\n",
      "2                    NO2       3.8650\n",
      "3                    NOx      -0.0000\n",
      "4                    NH3      -2.2527\n",
      "5                     CO       4.4040\n",
      "6                    SO2      -1.1540\n",
      "7                     O3       7.0237\n",
      "8       temperature_mean      -0.0000\n",
      "9        temperature_max      -0.4747\n",
      "10         humidity_mean       1.6766\n",
      "11          humidity_max       1.4253\n",
      "12        dew_point_mean       0.4842\n",
      "13         dew_point_max      -0.0000\n",
      "14     precipitation_sum      -0.0000\n",
      "15              rain_sum      -0.0000\n",
      "16       cloud_cover_sum      -0.7112\n",
      "17   wind_speed_10m_mean       1.3390\n",
      "18    wind_speed_10m_max      -0.2561\n",
      "19  wind_speed_100m_mean       0.1035\n",
      "20   wind_speed_100m_max       1.1126\n",
      "Cross-validated MSE (for each fold): [ 666.29561854 1825.46317522 1184.29753665  702.26193982  526.2527402 ]\n",
      "Average MSE: 980.9142020845122\n"
     ]
    }
   ],
   "source": [
    "# Using majority of the same code as above:\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = lasso.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(lasso.coef_,4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "lasso_cv = cross_val_score(lasso, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "                           \n",
    "# Output the results (MSE for each fold)\n",
    "print(\"Cross-validated MSE (for each fold):\", -lasso_cv)\n",
    "\n",
    "# Mean of the cross-validation scores\n",
    "print(\"Average MSE:\", -lasso_cv.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha for Lasso: 0.1\n",
      "Mean Squared Error: 621.5314242454493\n",
      "R-squared: 0.8341776618357627\n",
      "                 Feature  Coefficient\n",
      "0                    PC1      55.7205\n",
      "1                     NO       2.6738\n",
      "2                    NO2       3.8650\n",
      "3                    NOx      -0.0000\n",
      "4                    NH3      -2.2527\n",
      "5                     CO       4.4040\n",
      "6                    SO2      -1.1540\n",
      "7                     O3       7.0237\n",
      "8       temperature_mean      -0.0000\n",
      "9        temperature_max      -0.4747\n",
      "10         humidity_mean       1.6766\n",
      "11          humidity_max       1.4253\n",
      "12        dew_point_mean       0.4842\n",
      "13         dew_point_max      -0.0000\n",
      "14     precipitation_sum      -0.0000\n",
      "15              rain_sum      -0.0000\n",
      "16       cloud_cover_sum      -0.7112\n",
      "17   wind_speed_10m_mean       1.3390\n",
      "18    wind_speed_10m_max      -0.2561\n",
      "19  wind_speed_100m_mean       0.1035\n",
      "20   wind_speed_100m_max       1.1126\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Split into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the alpha values to test for tuning\n",
    "alpha_values = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]}\n",
    "\n",
    "# Initiate Ridge Regression and GridSearchCV\n",
    "lasso = Lasso()\n",
    "\n",
    "# Use GridSearchCV to tune alpha\n",
    "lasso_cv = GridSearchCV(lasso, alpha_values, cv=5, scoring='neg_mean_squared_error')\n",
    "lasso_cv.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Best alpha value\n",
    "best_alpha = lasso_cv.best_params_['alpha']\n",
    "print(f\"Best alpha for Lasso: {best_alpha}\")\n",
    "\n",
    "# Use the best alpha to fit the model again\n",
    "lasso_best = Lasso(alpha=best_alpha)\n",
    "lasso_best.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = lasso_best.predict(X_test_scaled)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Calculate R-squared\n",
    "r_squared = ridge_best.score(X_test_scaled, y_test)\n",
    "print(f\"R-squared: {r_squared}\")\n",
    "\n",
    "# Print Coefficients as table\n",
    "feature_names = X.columns\n",
    "coefficients = np.round(lasso_best.coef_, 4)\n",
    "\n",
    "coef_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Coefficient': coefficients\n",
    "})\n",
    "\n",
    "print(coef_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From here we can look to remove x,y and z features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based off of Ridge & Lasso Regression, we can remove certain components\n",
    "columns_to_drop = ['NOx', 'temperature_mean', 'dew_point_max', 'precipitation_sum', 'rain_sum', 'wind_speed_100m_mean', 'wind_speed_10m_max', 'temperature_max', 'dew_point_mean'] # Removing all zeroed components & any low coefficient features\n",
    "\n",
    "# Drop columns on original data\n",
    "#model_data = data_new.drop(columns_to_drop, axis=1) # This data does not include Principle Component Analysis\n",
    "\n",
    "# Apply Principle Component to original data and drop columns\n",
    "model_data = data_new.drop(['PM2.5', 'PM10'], axis=1)\n",
    "# Add PC1\n",
    "model_data.insert(2, 'PC1', pm_pca[:, 0])\n",
    "model_data.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Date</th>\n",
       "      <th>PC1</th>\n",
       "      <th>NO</th>\n",
       "      <th>NO2</th>\n",
       "      <th>NH3</th>\n",
       "      <th>CO</th>\n",
       "      <th>SO2</th>\n",
       "      <th>O3</th>\n",
       "      <th>AQI</th>\n",
       "      <th>humidity_mean</th>\n",
       "      <th>humidity_max</th>\n",
       "      <th>cloud_cover_sum</th>\n",
       "      <th>wind_speed_10m_mean</th>\n",
       "      <th>wind_speed_100m_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amaravati</td>\n",
       "      <td>2017-11-25</td>\n",
       "      <td>0.823902</td>\n",
       "      <td>1.44</td>\n",
       "      <td>20.5</td>\n",
       "      <td>10.72</td>\n",
       "      <td>0.12</td>\n",
       "      <td>15.24</td>\n",
       "      <td>127.09</td>\n",
       "      <td>184.0</td>\n",
       "      <td>64.906533</td>\n",
       "      <td>89.346016</td>\n",
       "      <td>819.000009</td>\n",
       "      <td>9.180352</td>\n",
       "      <td>27.248455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City       Date       PC1    NO   NO2    NH3    CO    SO2      O3  \\\n",
       "0  Amaravati 2017-11-25  0.823902  1.44  20.5  10.72  0.12  15.24  127.09   \n",
       "\n",
       "     AQI  humidity_mean  humidity_max  cloud_cover_sum  wind_speed_10m_mean  \\\n",
       "0  184.0      64.906533     89.346016       819.000009             9.180352   \n",
       "\n",
       "   wind_speed_100m_max  \n",
       "0            27.248455  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_data.to_csv('model_data.csv', index=True)\n",
    "\n",
    "model_data.head(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTER for Data Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='AQI', ylabel='Count'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of AQI\n",
    "sns.histplot(model_data['AQI'], bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dist_matrix: 100%|##########| 432/432 [00:29<00:00, 14.87it/s]\n",
      "synth_matrix: 100%|##########| 432/432 [00:01<00:00, 299.40it/s]\n",
      "r_index: 100%|##########| 233/233 [00:00<00:00, 909.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2628, 15) (3061, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "c:\\Users\\lukee\\anaconda3\\Lib\\site-packages\\smogn\\over_sampling.py:439: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0       Amaravati\n",
      "1       Amaravati\n",
      "2             1.0\n",
      "3             1.0\n",
      "4             2.0\n",
      "          ...    \n",
      "1092    Amaravati\n",
      "1093          2.0\n",
      "1094    Amaravati\n",
      "1095          2.0\n",
      "1096          2.0\n",
      "Name: 0, Length: 1097, dtype: object' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  data_new.iloc[:, j] = data_new.iloc[:, j].replace(x, cat_list[x])\n"
     ]
    }
   ],
   "source": [
    "import smogn\n",
    "\n",
    "# Implement SMOTER\n",
    "model_data_smoter = smogn.smoter(data=model_data,y='AQI')\n",
    "\n",
    "# Print sizes of dataframe\n",
    "print(model_data_smoter.shape, data_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='AQI', ylabel='Count'>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribtion of AQI after SMOTER\n",
    "sns.histplot(model_data_smoter['AQI'], bins=50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation Without SMOTER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Evaluation:\n",
    "- RMSE\n",
    "- MSE\n",
    "- R-Squared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark (Ordinary Least Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the data\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Perform cross-validation\n",
    "ols_cv = cross_val_score(linear_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "ols_cv_r2 = cross_val_score(linear_model, X, y, cv=5, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model performance\n",
    "ols_mse = mean_squared_error(y_test, y_pred)\n",
    "ols_rmse = np.sqrt(ols_mse)\n",
    "ols_r2 = r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Cross Validate\n",
    "rf_cv = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "rf_cv_r2 = cross_val_score(rf, X, y, cv=5, scoring='r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance\n",
    "rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "rf_r2 = r2_score(y_test, y_pred_rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning using RandomisedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'max_depth': 19, 'max_features': None, 'min_samples_leaf': 8, 'min_samples_split': 8, 'n_estimators': 84}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Prepare data\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),  # Random number of trees between 50 and 200\n",
    "    'max_depth': randint(5, 20),  # Random depth between 5 and 20\n",
    "    'min_samples_split': randint(2, 10),  # Random min samples for split between 2 and 10\n",
    "    'min_samples_leaf': randint(1, 10),  # Random min samples at leaf between 1 and 10\n",
    "    'max_features': ['sqrt', 'log2', None]  # Replace 'auto' with 'sqrt', 'log2', or None\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(rf, param_distributions=param_dist, \n",
    "                                   n_iter=20, cv=5, scoring='neg_mean_squared_error', \n",
    "                                   random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Access cross val results\n",
    "rf_results = random_search.cv_results_\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "rf_mse_best = mean_squared_error(y_test, y_pred_best_rf)\n",
    "rf_rmse_best = np.sqrt(rf_mse_best)\n",
    "rf_r2_best = r2_score(y_test, y_pred_best_rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data (SVR requires standardized input)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the Support Vector Regression model\n",
    "svr = SVR(kernel='rbf')  # Using RBF kernel for non-linear relationships\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_svr = svr.predict(X_test_scaled)\n",
    "\n",
    "# Cross Validate\n",
    "svr_cv = cross_val_score(svr, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "svr_cv_r2 = cross_val_score(svr, X, y, cv=5, scoring='r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "svr_mse = mean_squared_error(y_test, y_pred_svr)\n",
    "svr_rmse = np.sqrt(svr_mse)\n",
    "svr_r2 = r2_score(y_test, y_pred_svr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning using RandomisedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'C': 5.908361216819946, 'epsilon': 0.8761761457749352, 'gamma': 'auto', 'kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "# Prepare the data\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data (SVR requires standardized input)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Support Vector Regressor (SVR)\n",
    "svr = SVR()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'C': uniform(0.1, 100),  # Regularization parameter between 0.1 and 100\n",
    "    'epsilon': uniform(0.01, 1),  # Epsilon value for margin\n",
    "    'gamma': ['scale', 'auto'],  # Gamma parameter for the RBF kernel\n",
    "    'kernel': ['rbf', 'linear']  # Kernel options\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for tuning\n",
    "random_search_svr = RandomizedSearchCV(svr, param_distributions=param_dist, \n",
    "                                       n_iter=20, cv=5, scoring='neg_mean_squared_error', \n",
    "                                       random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search_svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Access cross validation results\n",
    "svr_results = random_search_svr.cv_results_\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_svr = random_search_svr.best_params_\n",
    "print(f\"Best parameters found: {best_params_svr}\")\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_svr = random_search_svr.best_estimator_\n",
    "y_pred_best_svr = best_svr.predict(X_test_scaled)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "svr_mse_best = mean_squared_error(y_test, y_pred_best_svr)\n",
    "svr_rmse_best = np.sqrt(svr_mse_best)\n",
    "svr_r2_best = r2_score(y_test, y_pred_best_svr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Prepare the data\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost Regressor with default or manually chosen parameters\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Perform cross-validation for MSE and R-squared\n",
    "xgb_cv_mse = cross_val_score(xgb_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "xgb_cv_r2 = cross_val_score(xgb_model, X, y, cv=5, scoring='r2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "xgb_mse = mean_squared_error(y_test, y_pred_xgb)\n",
    "xgb_rmse = np.sqrt(xgb_mse)\n",
    "xgb_r2 = r2_score(y_test, y_pred_xgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning using RandomisedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'colsample_bytree': 0.9162213204002109, 'learning_rate': 0.07370173320348283, 'max_depth': 6, 'n_estimators': 70, 'reg_alpha': 0.061748150962771656, 'reg_lambda': 0.7116531604882809, 'subsample': 0.5035331526098588}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Prepare the data\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),  # Number of trees (estimators)\n",
    "    'learning_rate': uniform(0.01, 0.3),  # Learning rate (eta)\n",
    "    'max_depth': randint(3, 10),  # Depth of trees\n",
    "    'subsample': uniform(0.5, 0.5),  # Subsampling of rows\n",
    "    'colsample_bytree': uniform(0.5, 0.5),  # Subsampling of columns\n",
    "    'reg_alpha': uniform(0, 0.1),  # L1 regularization (alpha)\n",
    "    'reg_lambda': uniform(0.1, 1.0),  # L2 regularization (lambda)\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for tuning\n",
    "random_search_xgb = RandomizedSearchCV(xgb_model, param_distributions=param_dist, \n",
    "                                       n_iter=20, cv=5, scoring='neg_mean_squared_error', \n",
    "                                       random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Access cross validation results\n",
    "xgb_results = random_search_xgb.cv_results_\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_xgb = random_search_xgb.best_params_\n",
    "print(f\"Best parameters found: {best_params_xgb}\")\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_xgb = random_search_xgb.best_estimator_\n",
    "y_pred_best_xgb = best_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "xgb_mse_best = mean_squared_error(y_test, y_pred_best_xgb)\n",
    "xgb_rmse_best = np.sqrt(xgb_mse_best)\n",
    "xgb_r2_best = r2_score(y_test, y_pred_best_xgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Neural Network + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Initialize XGBoost and Neural Network as base learners\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "nn_model = MLPRegressor(hidden_layer_sizes=(50,), activation='relu', solver='adam', max_iter=5000, random_state=42)\n",
    "\n",
    "# Create the stacking model using Linear Regression as the meta-model\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=[('xgb', xgb_model), ('nn', nn_model)],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the stacked model\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred_stacked = stacked_model.predict(X_test)\n",
    "\n",
    "hybrid_cv = cross_val_score(stacked_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "hybrid_cv_r2 = cross_val_score(stacked_model, X, y, cv=5, scoring='r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "stacked_mse = mean_squared_error(y_test, y_pred_stacked)\n",
    "stacked_rmse = np.sqrt(stacked_mse)\n",
    "stacked_r2 = r2_score(y_test, y_pred_stacked)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning using RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukee\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:698: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'nn__activation': 'relu', 'nn__alpha': 0.00299751452913768, 'nn__hidden_layer_sizes': (100,), 'nn__solver': 'adam', 'xgb__colsample_bytree': 0.9402339195076288, 'xgb__learning_rate': 0.197306214440138, 'xgb__max_depth': 8, 'xgb__n_estimators': 183, 'xgb__subsample': 0.7282672852414551}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Prepare data\n",
    "X = model_data.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize base models\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "nn_model = MLPRegressor(max_iter=5000, random_state=42, verbose=False)\n",
    "\n",
    "# Define the stacking model\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=[('xgb', xgb_model), ('nn', nn_model)],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': randint(50, 200),\n",
    "    'xgb__learning_rate': uniform(0.01, 0.3),\n",
    "    'xgb__max_depth': randint(3, 10),\n",
    "    'xgb__subsample': uniform(0.5, 0.5),  # Generates values strictly between 0.5 and 1.0\n",
    "    'xgb__colsample_bytree': uniform(0.5, 0.5),  # Same, strictly between 0.5 and 1.0\n",
    "    \n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (100, 100)],\n",
    "    'nn__activation': ['relu'],\n",
    "    'nn__alpha': uniform(0.0001, 0.01),  # Reasonable regularization range\n",
    "    'nn__solver': ['adam']\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=stacked_model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=20, \n",
    "                                   cv=5, \n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   random_state=42, \n",
    "                                   n_jobs=1,  # No parallel jobs\n",
    "                                   error_score='raise')  # Raise errors\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Access cross-validation results\n",
    "stacked_restults = random_search.cv_results_\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_stacked_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Make predictions and evaluate the tuned stacked model\n",
    "y_pred_best_stacked = best_stacked_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "stacked_mse_best = mean_squared_error(y_test, y_pred_best_stacked)\n",
    "stacked_rmse_best = np.sqrt(stacked_mse_best)\n",
    "stacked_r2_best = r2_score(y_test, y_pred_best_stacked)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 41.33337374302295\n",
      "P-value: 1.470636153878714e-07\n",
      "There is a statistically significant difference between the models.\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "rf_results_mse = rf_results['mean_test_score']\n",
    "svr_results_mse = svr_results['mean_test_score']\n",
    "xgb_results_mse = xgb_results['mean_test_score']\n",
    "stacked_restults_mse = stacked_restults['mean_test_score']\n",
    "\n",
    "# Perform ANOVA test\n",
    "f_statistic, p_value = f_oneway(rf_results, svr_results, xgb_results, stacked_restults)\n",
    "\n",
    "# Output the results\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretation of ANOVA test result\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference between the models.\")\n",
    "else:\n",
    "    print(\"No statistically significant difference between the models.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Multiple Comparison of Means - Tukey HSD, FWER=0.05           \n",
      "========================================================================\n",
      "    group1        group2     meandiff p-adj    lower      upper   reject\n",
      "------------------------------------------------------------------------\n",
      " Hybrid Model Random Forest   11.1165 0.9998 -1618.8031  1641.036  False\n",
      " Hybrid Model           SVG  3274.464 0.0005  1644.5445 4904.3835   True\n",
      "Random Forest           SVG 3263.3475 0.0005   1633.428 4893.2671   True\n",
      "------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Example MSE data for three models\n",
    "# Replace with your cross-validated MSE data\n",
    "mse_data = np.concatenate([mse_model1, mse_model2, mse_model3])\n",
    "\n",
    "# Corresponding labels for the models\n",
    "model_labels = (['Random Forest'] * len(mse_model1)) + (['SVG'] * len(mse_model2)) + (['Hybrid Model'] * len(mse_model3))\n",
    "\n",
    "# Create a DataFrame to hold the MSE values and corresponding model labels\n",
    "df = pd.DataFrame({'MSE': mse_data, 'Model': model_labels})\n",
    "\n",
    "# Perform Tukey's HSD test\n",
    "tukey = pairwise_tukeyhsd(endog=df['MSE'], groups=df['Model'], alpha=0.05)\n",
    "\n",
    "# Display the test results\n",
    "print(tukey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>RÂ²</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OLS</td>\n",
       "      <td>625.315772</td>\n",
       "      <td>25.006315</td>\n",
       "      <td>0.834104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>558.986912</td>\n",
       "      <td>23.642904</td>\n",
       "      <td>0.851701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVR</td>\n",
       "      <td>631.858721</td>\n",
       "      <td>25.136800</td>\n",
       "      <td>0.832368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGB</td>\n",
       "      <td>647.273404</td>\n",
       "      <td>25.441568</td>\n",
       "      <td>0.828279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGB + NN</td>\n",
       "      <td>649.325192</td>\n",
       "      <td>25.481860</td>\n",
       "      <td>0.827734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model         MSE       RMSE        RÂ²\n",
       "0            OLS  625.315772  25.006315  0.834104\n",
       "1  Random Forest  558.986912  23.642904  0.851701\n",
       "2            SVR  631.858721  25.136800  0.832368\n",
       "3            XGB  647.273404  25.441568  0.828279\n",
       "4       XGB + NN  649.325192  25.481860  0.827734"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe for Metric Comparison without SMOTER\n",
    "\n",
    "models = ['OLS', 'Random Forest', 'SVR', 'XGB', 'XGB + NN']\n",
    "mse = [ols_mse, rf_mse_best, svr_mse_best, xgb_mse_best, stacked_mse_best]\n",
    "rmse = [ols_rmse, rf_rmse_best, svr_rmse_best, xgb_rmse_best, stacked_rmse_best]\n",
    "r2 = [ols_r2, rf_r2_best, svr_r2_best, xgb_r2_best, stacked_r2_best]\n",
    "\n",
    "# Create the DataFrame\n",
    "df_results = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'RÂ²': r2\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "df_results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation with SMOTER"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark (Ordinary Least Squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLS Mean Squared Error: 1279.4880352463556\n",
      "OLS R-squared: 0.8578205292157414\n",
      "Cross-validated MSE: 1470.216785790645\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X = model_data_smoter.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data_smoter['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the model\n",
    "linear_model = LinearRegression()\n",
    "\n",
    "# Fit the model on the data\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = linear_model.predict(X_test)\n",
    "\n",
    "# Perform cross-validation\n",
    "ols_cv_smoter = cross_val_score(linear_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "ols_cv_r2_smoter = cross_val_score(linear_model, X, y, cv=5, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model performance\n",
    "ols_mse_smoter = mean_squared_error(y_test, y_pred)\n",
    "ols_rmse_smoter = np.sqrt(ols_mse)\n",
    "ols_r2_smoter = r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor MSE: 782.8713175619673\n",
      "Random Forest Regressor R-squared: 0.9130056502625253\n",
      "Cross-validated MSE (for each fold): [1390.14914262 1384.75425752  715.81075837 1341.72783844 1180.7648159 ]\n",
      "Average MSE: 1202.6413625682737\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X = model_data_smoter.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data_smoter['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and fit the Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "\n",
    "# Cross Validate\n",
    "rf_cv = cross_val_score(rf, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "rf_cv_r2 = cross_val_score(rf, X, y, cv=5, scoring='r2')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model Performance\n",
    "rf_mse_smoter = mean_squared_error(y_test, y_pred_rf)\n",
    "rf_rmse_smoter = np.sqrt(rf_mse_smoter)\n",
    "rf_r2_smoter = r2_score(y_test, y_pred_rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning using RandomisedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Prepare data\n",
    "X = model_data_smoter.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data_smoter['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the RandomForestRegressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),  # Random number of trees between 50 and 200\n",
    "    'max_depth': randint(5, 20),  # Random depth between 5 and 20\n",
    "    'min_samples_split': randint(2, 10),  # Random min samples for split between 2 and 10\n",
    "    'min_samples_leaf': randint(1, 10),  # Random min samples at leaf between 1 and 10\n",
    "    'max_features': ['sqrt', 'log2', None]  # Replace 'auto' with 'sqrt', 'log2', or None\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(rf, param_distributions=param_dist, \n",
    "                                   n_iter=20, cv=5, scoring='neg_mean_squared_error', \n",
    "                                   random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Access cross val results\n",
    "rf_results_smoter = random_search.cv_results_\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_rf = random_search.best_estimator_\n",
    "y_pred_best_rf = best_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "rf_mse_best_smoter = mean_squared_error(y_test, y_pred_best_rf)\n",
    "rf_rmse_best_smoter = np.sqrt(rf_mse_best_smoter)\n",
    "rf_r2_best_smoter = r2_score(y_test, y_pred_best_rf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = model_data_smoter.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data_smoter['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data (SVR requires standardized input)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and fit the Support Vector Regression model\n",
    "svr = SVR(kernel='rbf')  # Using RBF kernel for non-linear relationships\n",
    "svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_svr = svr.predict(X_test_scaled)\n",
    "\n",
    "# Cross Validate\n",
    "svr_cv = cross_val_score(svr, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "svr_cv_r2 = cross_val_score(svr, X, y, cv=5, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Model\n",
    "svr_mse_smoter = mean_squared_error(y_test, y_pred_svr)\n",
    "svr_rmse_smoter = np.sqrt(svr_mse_smoter)\n",
    "svr_r2_smoter = r2_score(y_test, y_pred_svr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning using RandomisedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "# Prepare the data\n",
    "X = model_data_smoter.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data_smoter['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the data (SVR requires standardized input)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize the Support Vector Regressor (SVR)\n",
    "svr = SVR()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'C': uniform(0.1, 100),  # Regularization parameter between 0.1 and 100\n",
    "    'epsilon': uniform(0.01, 1),  # Epsilon value for margin\n",
    "    'gamma': ['scale', 'auto'],  # Gamma parameter for the RBF kernel\n",
    "    'kernel': ['rbf', 'linear']  # Kernel options\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for tuning\n",
    "random_search_svr = RandomizedSearchCV(svr, param_distributions=param_dist, \n",
    "                                       n_iter=20, cv=5, scoring='neg_mean_squared_error', \n",
    "                                       random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search_svr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Access cross validation results\n",
    "svr_results_smoter = random_search_svr.cv_results_\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_svr = random_search_svr.best_params_\n",
    "print(f\"Best parameters found: {best_params_svr}\")\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_svr = random_search_svr.best_estimator_\n",
    "y_pred_best_svr = best_svr.predict(X_test_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the best model\n",
    "svr_mse_best_smoter = mean_squared_error(y_test, y_pred_best_svr)\n",
    "svr_rmse_best_smoter = np.sqrt(svr_mse_best_smoter)\n",
    "svr_r2_best_smoter = r2_score(y_test, y_pred_best_svr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Prepare the data\n",
    "X = model_data_smoter.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data_smoter['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost Regressor with default or manually chosen parameters\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Perform cross-validation for MSE and R-squared\n",
    "xgb_cv_mse = cross_val_score(xgb_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "xgb_cv_r2 = cross_val_score(xgb_model, X, y, cv=5, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "xgb_mse_smoter = mean_squared_error(y_test, y_pred_xgb)\n",
    "xgb_rmse_smoter = np.sqrt(xgb_mse_smoter)\n",
    "xgb_r2_smoter = r2_score(y_test, y_pred_xgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning using RandomisedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Prepare the data\n",
    "X = model_data_smoter.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data_smoter['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost Regressor\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_dist = {\n",
    "    'n_estimators': randint(50, 200),  # Number of trees (estimators)\n",
    "    'learning_rate': uniform(0.01, 0.3),  # Learning rate (eta)\n",
    "    'max_depth': randint(3, 10),  # Depth of trees\n",
    "    'subsample': uniform(0.5, 0.5),  # Subsampling of rows\n",
    "    'colsample_bytree': uniform(0.5, 0.5),  # Subsampling of columns\n",
    "    'reg_alpha': uniform(0, 0.1),  # L1 regularization (alpha)\n",
    "    'reg_lambda': uniform(0.1, 1.0),  # L2 regularization (lambda)\n",
    "}\n",
    "\n",
    "# Use RandomizedSearchCV for tuning\n",
    "random_search_xgb = RandomizedSearchCV(xgb_model, param_distributions=param_dist, \n",
    "                                       n_iter=20, cv=5, scoring='neg_mean_squared_error', \n",
    "                                       random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit the random search model\n",
    "random_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Access cross validation results\n",
    "xgb_results_smoter = random_search_xgb.cv_results_\n",
    "\n",
    "# Get the best parameters\n",
    "best_params_xgb = random_search_xgb.best_params_\n",
    "print(f\"Best parameters found: {best_params_xgb}\")\n",
    "\n",
    "# Use the best model to make predictions\n",
    "best_xgb = random_search_xgb.best_estimator_\n",
    "y_pred_best_xgb = best_xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "xgb_mse_best_smoter = mean_squared_error(y_test, y_pred_best_xgb)\n",
    "xgb_rmse_best_smoter = np.sqrt(xgb_mse_best_smoter)\n",
    "xgb_r2_best_smoter = r2_score(y_test, y_pred_best_xgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked Neural Network + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# Initialize XGBoost and Neural Network as base learners\n",
    "xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
    "nn_model = MLPRegressor(hidden_layer_sizes=(50,), activation='relu', solver='adam', max_iter=5000, random_state=42)\n",
    "\n",
    "# Create the stacking model using Linear Regression as the meta-model\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=[('xgb', xgb_model), ('nn', nn_model)],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "X = model_data_smoter.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data_smoter['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the stacked model\n",
    "stacked_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate the model\n",
    "y_pred_stacked = stacked_model.predict(X_test)\n",
    "\n",
    "hybrid_cv = cross_val_score(stacked_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "hybrid_cv_r2 = cross_val_score(stacked_model, X, y, cv=5, scoring='r2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "stacked_mse_smoter = mean_squared_error(y_test, y_pred_stacked)\n",
    "stacked_rmse_smoter = np.sqrt(stacked_mse_smoter)\n",
    "stacked_r2_smoter = r2_score(y_test, y_pred_stacked)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Tuning using RandomisedSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Prepare data\n",
    "X = model_data_smoter.drop(['City', 'Date', 'AQI'], axis=1)\n",
    "y = model_data_smoter['AQI']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize base models\n",
    "xgb_model = xgb.XGBRegressor(random_state=42)\n",
    "nn_model = MLPRegressor(max_iter=5000, random_state=42, verbose=False)\n",
    "\n",
    "# Define the stacking model\n",
    "stacked_model = StackingRegressor(\n",
    "    estimators=[('xgb', xgb_model), ('nn', nn_model)],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "# Define hyperparameter grid for RandomizedSearchCV\n",
    "param_grid = {\n",
    "    'xgb__n_estimators': randint(50, 200),\n",
    "    'xgb__learning_rate': uniform(0.01, 0.3),\n",
    "    'xgb__max_depth': randint(3, 10),\n",
    "    'xgb__subsample': uniform(0.5, 0.5),  # Generates values strictly between 0.5 and 1.0\n",
    "    'xgb__colsample_bytree': uniform(0.5, 0.5),  # Same, strictly between 0.5 and 1.0\n",
    "    \n",
    "    'nn__hidden_layer_sizes': [(50,), (100,), (100, 100)],\n",
    "    'nn__activation': ['relu'],\n",
    "    'nn__alpha': uniform(0.0001, 0.01),  # Reasonable regularization range\n",
    "    'nn__solver': ['adam']\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=stacked_model, \n",
    "                                   param_distributions=param_grid, \n",
    "                                   n_iter=20, \n",
    "                                   cv=5, \n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   random_state=42, \n",
    "                                   n_jobs=1,  # No parallel jobs\n",
    "                                   error_score='raise')  # Raise errors\n",
    "\n",
    "# Fit the random search model\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Access cross-validation results\n",
    "stacked_restults_smoter = random_search.cv_results_\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_stacked_model = random_search.best_estimator_\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Make predictions and evaluate the tuned stacked model\n",
    "y_pred_best_stacked = best_stacked_model.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "stacked_mse_best_smoter = mean_squared_error(y_test, y_pred_best_stacked)\n",
    "stacked_rmse_best_smoter = np.sqrt(stacked_mse_best_smoter)\n",
    "stacked_r2_best_smoter = r2_score(y_test, y_pred_best_stacked)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "rf_results_mse_smoter = rf_results_smoter['mean_test_score']\n",
    "svr_results_mse_smoter = svr_results_smoter['mean_test_score']\n",
    "xgb_results_mse_smoter = xgb_results_smoter['mean_test_score']\n",
    "stacked_restults_mse_smoter = stacked_restults_smoter['mean_test_score']\n",
    "\n",
    "# Perform ANOVA test\n",
    "f_statistic, p_value = f_oneway(rf_results_mse_smoter, svr_results_mse_smoter, xgb_results_mse_smoter, stacked_restults_mse_smoter)\n",
    "\n",
    "# Output the results\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Interpretation of ANOVA test result\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference between the models.\")\n",
    "else:\n",
    "    print(\"No statistically significant difference between the models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Example MSE data for three models\n",
    "# Replace with your cross-validated MSE data\n",
    "mse_data = np.concatenate([mse_model1, mse_model2, mse_model3])\n",
    "\n",
    "# Corresponding labels for the models\n",
    "model_labels = (['Random Forest'] * len(mse_model1)) + (['SVG'] * len(mse_model2)) + (['Hybrid Model'] * len(mse_model3))\n",
    "\n",
    "# Create a DataFrame to hold the MSE values and corresponding model labels\n",
    "df = pd.DataFrame({'MSE': mse_data, 'Model': model_labels})\n",
    "\n",
    "# Perform Tukey's HSD test\n",
    "tukey = pairwise_tukeyhsd(endog=df['MSE'], groups=df['Model'], alpha=0.05)\n",
    "\n",
    "# Display the test results\n",
    "print(tukey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for Metric Comparison without SMOTER\n",
    "\n",
    "models = ['OLS', 'Random Forest', 'SVR', 'XGB', 'XGB + NN']\n",
    "mse = [ols_mse_smoter, rf_mse_best_smoter, svr_mse_best_smoter, xgb_mse_best_smoter, stacked_mse_best_smoter]\n",
    "rmse = [ols_rmse_smoter, rf_rmse_best_smoter, svr_rmse_best_smoter, xgb_rmse_best_smoter, stacked_rmse_best_smoter]\n",
    "r2 = [ols_r2_smoter, rf_r2_best_smoter, svr_r2_best_smoter, xgb_r2_best_smoter, stacked_r2_best_smoter]\n",
    "\n",
    "# Create the DataFrame\n",
    "df_results_smoter = pd.DataFrame({\n",
    "    'Model': models,\n",
    "    'MSE': mse,\n",
    "    'RMSE': rmse,\n",
    "    'RÂ²': r2\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "df_results_smoter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE vs Without SMOTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
